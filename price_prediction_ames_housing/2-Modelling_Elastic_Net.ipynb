{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Modelling with Elastic Net\n",
    "Build a pipeline to model an optimized Elastic Net solution.\n",
    "Evaluate Feature Importances.\n",
    "\n",
    "**Data Sources**\n",
    "\n",
    "- `data/raw/train.csv`: Training set from [kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).\n",
    "\n",
    "**Changes**\n",
    "\n",
    "- 2019-03-22: Start notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-libraries,-load-data\" data-toc-modified-id=\"Import-libraries,-load-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import libraries, load data</a></span></li><li><span><a href=\"#Go-quick-&amp;-dirty\" data-toc-modified-id=\"Go-quick-&amp;-dirty-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Go quick &amp; dirty</a></span></li><li><span><a href=\"#Pre-process-data-(outside-of-sklearn-pipeline)\" data-toc-modified-id=\"Pre-process-data-(outside-of-sklearn-pipeline)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pre-process data (outside of sklearn pipeline)</a></span><ul class=\"toc-item\"><li><span><a href=\"#General-pre-processing\" data-toc-modified-id=\"General-pre-processing-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>General pre-processing</a></span></li><li><span><a href=\"#Create-two-versions-of-training-data-for-different-outlier-treatment-(experiment)\" data-toc-modified-id=\"Create-two-versions-of-training-data-for-different-outlier-treatment-(experiment)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Create two versions of training data for different outlier treatment (experiment)</a></span></li><li><span><a href=\"#Split-train-&amp;-test-sets\" data-toc-modified-id=\"Split-train-&amp;-test-sets-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Split train &amp; test sets</a></span></li></ul></li><li><span><a href=\"#Fit,-tune,-predict-(with-Pipelines)\" data-toc-modified-id=\"Fit,-tune,-predict-(with-Pipelines)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Fit, tune, predict (with Pipelines)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-Pipe\" data-toc-modified-id=\"Build-Pipe-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Build Pipe</a></span></li><li><span><a href=\"#Fit-&amp;-Tune\" data-toc-modified-id=\"Fit-&amp;-Tune-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Fit &amp; Tune</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries, load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:28.849846Z",
     "start_time": "2019-03-25T22:27:27.470055Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# My functions\n",
    "import EDA_functions as EDA\n",
    "import cleaning_functions as cleaning\n",
    "from linRegModel_class import LinRegModel\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #, sns.set_style('whitegrid')\n",
    "color = 'rebeccapurple'\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:28.899830Z",
     "start_time": "2019-03-25T22:27:28.849846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "raw_data = pd.read_csv('data/raw/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:28.911166Z",
     "start_time": "2019-03-25T22:27:28.899830Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load variables from notebook 1\n",
    "%store -r cols_to_del\n",
    "%store -r cols_to_log\n",
    "%store -r outliers_to_del\n",
    "%store -r top_corr_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go quick & dirty\n",
    "Use my 'quick & dirty' function for a baseline model on unprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:28.928662Z",
     "start_time": "2019-03-25T22:27:28.911166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=[0.5, 0.1, 1.5], copy_X=True, cv=5, eps=0.001,\n",
       "       fit_intercept=True, l1_ratio=0.5, max_iter=2000, n_alphas=None,\n",
       "       n_jobs=-1, normalize=False, positive=False, precompute='auto',\n",
       "       random_state=None, selection='cyclic', tol=0.0001, verbose=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize a scikit-learn model object of choice\n",
    "model_simple = ElasticNetCV(alphas=[0.5, 0.1, 1.5], copy_X=True, cv=5, eps=0.001, \n",
    "                            fit_intercept=True, l1_ratio=0.5, max_iter=2000, \n",
    "                            n_alphas=None, n_jobs=-1)\n",
    "\n",
    "# Create an instance of the LinRegModel class by passing df, target variable and model object\n",
    "elastic_net_simple = LinRegModel(raw_data, 'SalePrice', model_simple)\n",
    "\n",
    "# Output instance\n",
    "display(elastic_net_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:30.677918Z",
     "start_time": "2019-03-25T22:27:28.928662Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Perform the modelling\n",
    "elastic_net_simple.go_quickDirty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:30.689916Z",
     "start_time": "2019-03-25T22:27:30.681916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=[0.5, 0.1, 1.5], copy_X=True, cv=5, eps=0.001,\n",
       "       fit_intercept=True, l1_ratio=0.5, max_iter=2000, n_alphas=None,\n",
       "       n_jobs=-1, normalize=False, positive=False, precompute='auto',\n",
       "       random_state=None, selection='cyclic', tol=0.0001, verbose=0)\n",
       "\n",
       "RMSE on test data 34631.39, r2-score 0.79."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output result\n",
    "elastic_net_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:30.704970Z",
     "start_time": "2019-03-25T22:27:30.689916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check best alpha value\n",
    "model_simple.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data (outside of sklearn pipeline)\n",
    "Pre-processing steps that take place before data is pipelined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T20:48:57.218638Z",
     "start_time": "2019-03-25T20:48:57.214648Z"
    }
   },
   "source": [
    "### General pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:30.993199Z",
     "start_time": "2019-03-25T22:27:30.704970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alley successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Id successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Fence successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PoolQC successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'FireplaceQu successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'MiscFeature successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Disable warning\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# Create and clean training set with variables from the EDA notebook\n",
    "train_data = (raw_data\n",
    "              .pipe(cleaning.change_dtypes, cols_to_category=raw_data.select_dtypes(object))\n",
    "              .pipe(cleaning.delete_columns, cols_to_delete=cols_to_del)\n",
    "              .pipe(cleaning.apply_log, cols_to_transform=cols_to_log)\n",
    "             )\n",
    "\n",
    "train_data.drop(outliers_to_del, inplace=True)\n",
    "train_data.dropna(subset=['MasVnrArea', 'MasVnrType', 'Electrical'], inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T20:56:26.299332Z",
     "start_time": "2019-03-25T20:56:26.295343Z"
    }
   },
   "source": [
    "### Create two versions of training data for different outlier treatment (experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:31.049793Z",
     "start_time": "2019-03-25T22:27:30.999737Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create List of Columns containing NaN\n",
    "nan_cols = []\n",
    "for col in train_data.columns:\n",
    "    if train_data[col].isnull().sum() > 0:\n",
    "        nan_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:31.059073Z",
     "start_time": "2019-03-25T22:27:31.049793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LotFrontage',\n",
       " 'BsmtQual',\n",
       " 'BsmtCond',\n",
       " 'BsmtExposure',\n",
       " 'BsmtFinType1',\n",
       " 'BsmtFinType2',\n",
       " 'GarageType',\n",
       " 'GarageYrBlt',\n",
       " 'GarageFinish',\n",
       " 'GarageQual',\n",
       " 'GarageCond']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check results\n",
    "nan_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T20:47:49.812813Z",
     "start_time": "2019-03-25T20:47:49.805835Z"
    }
   },
   "source": [
    "**Note:** All remaining cols with missing values but Lot Frontage are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:31.071603Z",
     "start_time": "2019-03-25T22:27:31.060090Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create train set without missing values\n",
    "train_data_reduced = train_data.drop(nan_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:31.088140Z",
     "start_time": "2019-03-25T22:27:31.072623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set with NaN:  75\n",
      "train set without NaN:  64\n"
     ]
    }
   ],
   "source": [
    "# Check results\n",
    "print(\"train set with NaN: \", train_data.shape[1])\n",
    "print(\"train set without NaN: \", train_data_reduced.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T22:31:35.741110Z",
     "start_time": "2019-03-22T22:31:35.737110Z"
    }
   },
   "source": [
    "### Split train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:31.104139Z",
     "start_time": "2019-03-25T22:27:31.088140Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set with NaN\n",
    "X_train = train_data.drop('SalePrice', axis=1)\n",
    "y_train = train_data['SalePrice'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:31.156138Z",
     "start_time": "2019-03-25T22:27:31.104139Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = X_train.select_dtypes(include=['category']).columns\n",
    "numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:31.172138Z",
     "start_time": "2019-03-25T22:27:31.164138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categorical_features) + len(numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:31.188139Z",
     "start_time": "2019-03-25T22:27:31.176138Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set without NaN\n",
    "X_train_reduced = train_data_reduced.drop('SalePrice', axis=1)\n",
    "y_train_reduced = train_data_reduced['SalePrice'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:31.225480Z",
     "start_time": "2019-03-25T22:27:31.188139Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features_reduced = X_train_reduced.select_dtypes(include=['category']).columns\n",
    "numeric_features_reduced = X_train_reduced.select_dtypes(include=['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:31.233481Z",
     "start_time": "2019-03-25T22:27:31.225480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categorical_features_reduced) + len(numeric_features_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit, tune, predict (with Pipelines)\n",
    "\n",
    "### Build Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:31.265480Z",
     "start_time": "2019-03-25T22:27:31.237483Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assemble pipeline (define function)\n",
    "def build_pipe(X_train, y_train, numeric_features, categorical_features, clf):\n",
    "    \"\"\"Build a pipeline for preprocessing and modelling.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "        X_train: training features (df or array)\n",
    "        y_train: training labels (df or array)\n",
    "        numeric_features: list of strings, numeric columns\n",
    "        categorical_features: list of strings, categorical columns\n",
    "        clf: classifier (sk-learn model object)\n",
    "        \n",
    "    RETURNS:\n",
    "        full_pipe: pipeline object\n",
    "    \"\"\"\n",
    "    # level 1 - two separate pipes for cat and num features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer_n', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "            ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer_c', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore')),\n",
    "            ])\n",
    "\n",
    "    # level 2 - wrap the two level 1 pipes into a ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features),\n",
    "                         ])\n",
    "\n",
    "    # level 3 - pipe it with a classifier\n",
    "    full_pipe = Pipeline(steps=[\n",
    "                       ('preprocessor', preprocessor),\n",
    "                       ('clf', model_simple),\n",
    "                               ]) \n",
    "    \n",
    "    return full_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:31.277488Z",
     "start_time": "2019-03-25T22:27:31.265480Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "full_pipe = build_pipe(X_train, y_train, numeric_features, \n",
    "                       categorical_features, model_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:31.289482Z",
     "start_time": "2019-03-25T22:27:31.281480Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build pipeline for train set without NaN\n",
    "full_pipe_reduced = build_pipe(X_train_reduced, y_train_reduced, \n",
    "                               numeric_features_reduced, \n",
    "                               categorical_features_reduced, model_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T21:37:13.598677Z",
     "start_time": "2019-03-25T21:37:13.593531Z"
    }
   },
   "source": [
    "### Fit & Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:31.309480Z",
     "start_time": "2019-03-25T22:27:31.289482Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_pipe(X_train, y_train, pipe, scorer, cv=StratifiedKFold(3)):\n",
    "    \"\"\"Fit training data to a pipeline with GridSearchCV\n",
    "    for best parameter tuning.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "        X_train: training features (df or array)\n",
    "        y_train: training labels (df or array)\n",
    "        pipe: pipeline (sk-learn pipeline object)\n",
    "        scorer: evaluation metric for validation\n",
    "        cv: type of CV, default is StratifiedKFold(3)\n",
    "        \n",
    "    RETURNS:\n",
    "        grid: grid search object\n",
    "        grid_results: dict with grid search results\n",
    "    \"\"\"\n",
    "    parameters = {\n",
    "            'preprocessor__num__imputer_n__strategy': ['mean', 'median'],\n",
    "#             'classifier__C': [0.1, 1.0, 10, 100],\n",
    "\n",
    "                 }\n",
    "\n",
    "    cv = GridSearchCV(pipe, param_grid=parameters, scoring=scorer, n_jobs=-1, iid=False,\n",
    "                      cv=cv, error_score='raise', return_train_score=False, verbose=1)\n",
    "\n",
    "    grid = cv.fit(X_train, y_train) \n",
    "    grid_results = grid.cv_results_\n",
    "\n",
    "    return grid, grid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:40.129777Z",
     "start_time": "2019-03-25T22:27:31.309480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    7.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    7.3s finished\n"
     ]
    }
   ],
   "source": [
    "scorer = scorer = make_scorer(mean_squared_error)\n",
    "cv = 3\n",
    "\n",
    "# Pipe with NaN\n",
    "grid, grid_results = fit_pipe(X_train, y_train, full_pipe, scorer, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:40.141915Z",
     "start_time": "2019-03-25T22:27:40.129777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('num', Pipeline(memory=None,\n",
       "     steps=[('imputer_n', SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='mean',\n",
       "       verbo...ive=False, precompute='auto',\n",
       "       random_state=None, selection='cyclic', tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:40.149916Z",
     "start_time": "2019-03-25T22:27:40.141915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "822119404.750297"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:43.859814Z",
     "start_time": "2019-03-25T22:27:40.149916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    2.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    2.8s finished\n"
     ]
    }
   ],
   "source": [
    "# Pipe without NaN\n",
    "grid_reduced, grid_results_reduced = fit_pipe(X_train_reduced, y_train_reduced, \n",
    "                                              full_pipe_reduced, scorer, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:28:11.666899Z",
     "start_time": "2019-03-25T22:28:11.652378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "905441377.1598457"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_reduced.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T21:21:11.893729Z",
     "start_time": "2019-03-25T21:21:11.872393Z"
    }
   },
   "source": [
    "Includes following of the pre-processing steps identified in notebook 1 because I want them to be evaluated:\n",
    "\n",
    "- watch multicollinearity (evtl. remove cols: '1stFloor', 'GarageArea', 'FirstFlSF')\n",
    "- test IQR-method on 'top_corr_columns' as alternative\n",
    "- one-hot-encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T22:27:44.105725Z",
     "start_time": "2019-03-25T22:27:43.859814Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OutlierDropperIQR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-de7a371673d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m first_transformer = Pipeline(steps=[\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[1;34m'crop'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOutlierDropperIQR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcols_to_crop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#     ('drop', ColumnDropper(columns=cols_to_del_multicol)),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OutlierDropperIQR' is not defined"
     ]
    }
   ],
   "source": [
    "# # OLD PIPE\n",
    "\n",
    "# cols_to_crop = top_corr_columns[1:]  # 'SalePrice' has to be dropped\n",
    "# cols_to_del_multicol = ['1stFlrSF', 'GarageArea', 'TotRmsAbvGrd', 'GarageYrBlt']\n",
    "\n",
    "\n",
    "# first_transformer = Pipeline(steps=[\n",
    "#     ('crop', OutlierDropperIQR(columns=cols_to_crop)),\n",
    "# #     ('drop', ColumnDropper(columns=cols_to_del_multicol)),\n",
    "#     ])\n",
    "\n",
    "# # level 1 - two separate pipes for cat and num features\n",
    "\n",
    "# numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('scaler', StandardScaler())])\n",
    "\n",
    "# categorical_features = X_train.select_dtypes(include=['category']).columns\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# # level 2 - wrap the two level 1 pipes into a ColumnTransformer\n",
    "# preprocessor = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             ('num', numeric_transformer, numeric_features),\n",
    "#             ('cat', categorical_transformer, categorical_features),\n",
    "#                      ])\n",
    "\n",
    "# # level 3 - pipe it with a classifier\n",
    "# clf = Pipeline(steps=[\n",
    "#                    ('first', first_transformer),\n",
    "#                    ('preprocessor', preprocessor),\n",
    "#                    ('regressor', model_simple),\n",
    "#                      ]) \n",
    "\n",
    "# # apply the preprocessor and then pass transformed data to the predictor \n",
    "# clf.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "464.86px",
    "left": "1092.86px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
