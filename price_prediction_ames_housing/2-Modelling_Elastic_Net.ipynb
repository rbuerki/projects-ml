{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Modelling with Elastic Net\n",
    "Build a pipeline to model an optimized Elastic Net solution.\n",
    "Evaluate Feature Importances.\n",
    "\n",
    "**Data Sources**\n",
    "\n",
    "- `data/raw/train.csv`: Training set from [kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).\n",
    "\n",
    "**Changes**\n",
    "\n",
    "- 2019-03-22: Start notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-libraries,-load-data\" data-toc-modified-id=\"Import-libraries,-load-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import libraries, load data</a></span></li><li><span><a href=\"#Go-quick-&amp;-dirty\" data-toc-modified-id=\"Go-quick-&amp;-dirty-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Go quick &amp; dirty</a></span></li><li><span><a href=\"#Pre-process-data-(outside-of-sklearn-pipeline)\" data-toc-modified-id=\"Pre-process-data-(outside-of-sklearn-pipeline)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pre-process data (outside of sklearn pipeline)</a></span><ul class=\"toc-item\"><li><span><a href=\"#General-pre-processing\" data-toc-modified-id=\"General-pre-processing-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>General pre-processing</a></span></li><li><span><a href=\"#Create-two-versions-of-training-data-for-different-outlier-treatment-(experiment)\" data-toc-modified-id=\"Create-two-versions-of-training-data-for-different-outlier-treatment-(experiment)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Create two versions of training data for different outlier treatment (experiment)</a></span></li><li><span><a href=\"#Split-train-&amp;-test-sets\" data-toc-modified-id=\"Split-train-&amp;-test-sets-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Split train &amp; test sets</a></span></li></ul></li><li><span><a href=\"#Fit,-tune,-predict-(with-Pipelines)\" data-toc-modified-id=\"Fit,-tune,-predict-(with-Pipelines)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Fit, tune, predict (with Pipelines)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-Pipe\" data-toc-modified-id=\"Build-Pipe-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Build Pipe</a></span></li><li><span><a href=\"#Fit-&amp;-Tune\" data-toc-modified-id=\"Fit-&amp;-Tune-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Fit &amp; Tune</a></span><ul class=\"toc-item\"><li><span><a href=\"#Explore-NaN-Handling\" data-toc-modified-id=\"Explore-NaN-Handling-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Explore NaN-Handling</a></span></li><li><span><a href=\"#Explore-Outlier-Handling\" data-toc-modified-id=\"Explore-Outlier-Handling-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Explore Outlier-Handling</a></span></li><li><span><a href=\"#Explore-Multicollinearity-Removal\" data-toc-modified-id=\"Explore-Multicollinearity-Removal-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Explore Multicollinearity-Removal</a></span></li></ul></li><li><span><a href=\"#Final-Tuning-&amp;-Evaluation\" data-toc-modified-id=\"Final-Tuning-&amp;-Evaluation-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Final Tuning &amp; Evaluation</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries, load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:02.673401Z",
     "start_time": "2019-03-28T16:47:00.731608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# My functions\n",
    "import EDA_functions as EDA\n",
    "import cleaning_functions as cleaning\n",
    "from linRegModel_class import LinRegModel\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #, sns.set_style('whitegrid')\n",
    "color = 'rebeccapurple'\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:02.731559Z",
     "start_time": "2019-03-28T16:47:02.677732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "raw_data = pd.read_csv('data/raw/train.csv')\n",
    "\n",
    "# Check shape\n",
    "display(raw_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:02.743616Z",
     "start_time": "2019-03-28T16:47:02.733681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load variables from notebook 1\n",
    "%store -r cols_to_del\n",
    "%store -r cols_to_log\n",
    "%store -r outliers_to_del\n",
    "%store -r top_corr_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go quick & dirty\n",
    "Use my 'quick & dirty' function for a baseline model on unprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:02.765921Z",
     "start_time": "2019-03-28T16:47:02.745863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=[0.5, 0.1, 1.5], copy_X=True, cv=5, eps=0.001,\n",
       "       fit_intercept=True, l1_ratio=[0.2, 0.5, 0.8], max_iter=2000,\n",
       "       n_alphas=100, n_jobs=-1, normalize=False, positive=False,\n",
       "       precompute='auto', random_state=None, selection='cyclic',\n",
       "       tol=0.0001, verbose=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize a scikit-learn model object of choice\n",
    "model_simple = ElasticNetCV(alphas=[0.5, 0.1, 1.5], copy_X=True, cv=5, eps=0.001, \n",
    "                            fit_intercept=True, l1_ratio=[0.2, 0.5, 0.8], max_iter=2000, \n",
    "                            n_alphas=100, n_jobs=-1)\n",
    "\n",
    "# Create an instance of the LinRegModel class by passing df, target variable and model object\n",
    "elastic_net_simple = LinRegModel(raw_data, 'SalePrice', model_simple)\n",
    "\n",
    "# Output instance\n",
    "display(elastic_net_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:07.614536Z",
     "start_time": "2019-03-28T16:47:02.767853Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Perform the modelling\n",
    "elastic_net_simple.go_quickDirty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:07.637471Z",
     "start_time": "2019-03-28T16:47:07.619679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=[0.5, 0.1, 1.5], copy_X=True, cv=5, eps=0.001,\n",
       "       fit_intercept=True, l1_ratio=[0.2, 0.5, 0.8], max_iter=2000,\n",
       "       n_alphas=100, n_jobs=-1, normalize=False, positive=False,\n",
       "       precompute='auto', random_state=None, selection='cyclic',\n",
       "       tol=0.0001, verbose=0)\n",
       "\n",
       "RMSE on test data 33771.86, r2-score 0.80."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output result\n",
    "elastic_net_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:07.669181Z",
     "start_time": "2019-03-28T16:47:07.644663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.8\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "# Check best alpha value\n",
    "print(model_simple.alpha_)\n",
    "print(model_simple.l1_ratio_)\n",
    "print(model_simple.n_iter_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data (outside of sklearn pipeline)\n",
    "Pre-processing steps that take place before data is pipelined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T20:48:57.218638Z",
     "start_time": "2019-03-25T20:48:57.214648Z"
    }
   },
   "source": [
    "### General pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:07.983855Z",
     "start_time": "2019-03-28T16:47:07.670179Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MiscFeature successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PoolQC successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'FireplaceQu successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Alley successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Id successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Fence successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Disable warning\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# Create and clean training set with variables from the EDA notebook\n",
    "train_data = (raw_data\n",
    "              .pipe(cleaning.change_dtypes, cols_to_category=raw_data.select_dtypes(object))\n",
    "              .pipe(cleaning.delete_columns, cols_to_delete=cols_to_del)\n",
    "              .pipe(cleaning.apply_log, cols_to_transform=cols_to_log)\n",
    "             )\n",
    "\n",
    "train_data.drop(outliers_to_del, inplace=True)\n",
    "train_data.dropna(subset=['MasVnrArea', 'MasVnrType', 'Electrical'], inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:08.003966Z",
     "start_time": "2019-03-28T16:47:07.983855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1447, 75)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check results\n",
    "display(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T20:56:26.299332Z",
     "start_time": "2019-03-25T20:56:26.295343Z"
    }
   },
   "source": [
    "### Create two versions of training data for different outlier treatment (experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:08.078700Z",
     "start_time": "2019-03-28T16:47:08.013788Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create List of Columns containing NaN\n",
    "nan_cols = []\n",
    "for col in train_data.columns:\n",
    "    if train_data[col].isnull().sum() > 0:\n",
    "        nan_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:08.099358Z",
     "start_time": "2019-03-28T16:47:08.079695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LotFrontage',\n",
       " 'BsmtQual',\n",
       " 'BsmtCond',\n",
       " 'BsmtExposure',\n",
       " 'BsmtFinType1',\n",
       " 'BsmtFinType2',\n",
       " 'GarageType',\n",
       " 'GarageYrBlt',\n",
       " 'GarageFinish',\n",
       " 'GarageQual',\n",
       " 'GarageCond']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check results\n",
    "nan_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T20:47:49.812813Z",
     "start_time": "2019-03-25T20:47:49.805835Z"
    }
   },
   "source": [
    "**Note:** All remaining cols with missing values but Lot Frontage are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:08.121486Z",
     "start_time": "2019-03-28T16:47:08.105207Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create train set without missing values (drop nan_cols)\n",
    "train_data_reduced = train_data.drop(nan_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:08.174718Z",
     "start_time": "2019-03-28T16:47:08.125173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set with NaN:  75\n",
      "train set without NaN:  64\n"
     ]
    }
   ],
   "source": [
    "# Check results - compare the two training sets\n",
    "print(\"train set with NaN: \", train_data.shape[1])\n",
    "print(\"train set without NaN: \", train_data_reduced.shape[1])\n",
    "\n",
    "assert train_data_reduced.isnull().sum().sum() == 0\n",
    "assert train_data_reduced.shape[1] == train_data.shape[1] - len(nan_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T22:31:35.741110Z",
     "start_time": "2019-03-22T22:31:35.737110Z"
    }
   },
   "source": [
    "### Split train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:08.190700Z",
     "start_time": "2019-03-28T16:47:08.179701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set with NaN\n",
    "X_train = train_data.drop('SalePrice', axis=1)\n",
    "y_train = train_data['SalePrice'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:08.250360Z",
     "start_time": "2019-03-28T16:47:08.196896Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = X_train.select_dtypes(include=['category']).columns\n",
    "numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "assert len(categorical_features) + len(numeric_features) == train_data.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:08.268012Z",
     "start_time": "2019-03-28T16:47:08.253200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set without NaN\n",
    "X_train_reduced = train_data_reduced.drop('SalePrice', axis=1)\n",
    "y_train_reduced = train_data_reduced['SalePrice'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:08.319733Z",
     "start_time": "2019-03-28T16:47:08.270730Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features_reduced = X_train_reduced.select_dtypes(include=['category']).columns\n",
    "numeric_features_reduced = X_train_reduced.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "assert len(categorical_features_reduced) + len(numeric_features_reduced) \\\n",
    "        == train_data_reduced.shape[1] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit, tune, predict (with Pipelines)\n",
    "\n",
    "### Build Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:08.355482Z",
     "start_time": "2019-03-28T16:47:08.322729Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assemble pipeline (define function)\n",
    "def build_pipe(X_train, y_train, numeric_features, categorical_features, clf):\n",
    "    \"\"\"Build a pipeline for preprocessing and modelling.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "        X_train: training features (df or array)\n",
    "        y_train: training labels (df or array)\n",
    "        numeric_features: list of strings, numeric columns\n",
    "        categorical_features: list of strings, categorical columns\n",
    "        clf: classifier (sk-learn model object)\n",
    "        \n",
    "    RETURNS:\n",
    "        full_pipe: pipeline object\n",
    "    \"\"\"\n",
    "    # level 1 - two separate pipes for cat and num features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer_n', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "            ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer_c', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore')),\n",
    "            ])\n",
    "\n",
    "    # level 2 - wrap the two level 1 pipes into a ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features),\n",
    "                         ])\n",
    "\n",
    "    # level 3 - pipe it with a classifier\n",
    "    full_pipe = Pipeline(steps=[\n",
    "                       ('preprocessor', preprocessor),\n",
    "                       ('clf', model_simple),\n",
    "                               ]) \n",
    "    \n",
    "    return full_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:08.383547Z",
     "start_time": "2019-03-28T16:47:08.362749Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "full_pipe = build_pipe(X_train, y_train, numeric_features, \n",
    "                       categorical_features, \n",
    "                       elastic_net_simple) # optimized model from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:08.395534Z",
     "start_time": "2019-03-28T16:47:08.387208Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build pipeline for train set without NaN\n",
    "full_pipe_reduced = build_pipe(X_train_reduced, y_train_reduced, \n",
    "                               numeric_features_reduced, \n",
    "                               categorical_features_reduced, \n",
    "                               elastic_net_simple) # optimized model from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T21:37:13.598677Z",
     "start_time": "2019-03-25T21:37:13.593531Z"
    }
   },
   "source": [
    "### Fit & Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T15:32:54.351660Z",
     "start_time": "2019-03-28T15:32:54.343625Z"
    }
   },
   "source": [
    "#### Explore NaN-Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:08.438001Z",
     "start_time": "2019-03-28T16:47:08.401991Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_pipe(X_train, y_train, pipe, scorer, cv=StratifiedKFold(3)):\n",
    "    \"\"\"Fit training data to a pipeline with GridSearchCV\n",
    "    for best parameter tuning.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "        X_train: training features (df or array)\n",
    "        y_train: training labels (df or array)\n",
    "        pipe: pipeline (sk-learn pipeline object)\n",
    "        scorer: evaluation metric for validation\n",
    "        cv: type of CV, default is StratifiedKFold(3)\n",
    "        \n",
    "    RETURNS:\n",
    "        grid: grid search object\n",
    "        grid_results: dict with grid search results\n",
    "    \"\"\"\n",
    "    parameters = {\n",
    "            'preprocessor__num__imputer_n__strategy': ['mean', 'median'],\n",
    "            'preprocessor__num__scaler' : [None, StandardScaler()]\n",
    "#             'classifier__C': [0.1, 1.0, 10, 100],\n",
    "\n",
    "                 }\n",
    "\n",
    "    cv = GridSearchCV(pipe, param_grid=parameters, scoring=scorer, n_jobs=-1, iid=False,\n",
    "                      cv=cv, error_score='raise', return_train_score=False, verbose=1)\n",
    "\n",
    "    grid = cv.fit(X_train, y_train) \n",
    "    grid_results = grid.cv_results_\n",
    "\n",
    "    return grid, grid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:19.681274Z",
     "start_time": "2019-03-28T16:47:08.441669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:   10.4s finished\n"
     ]
    }
   ],
   "source": [
    "scorer = make_scorer(mean_squared_error)\n",
    "cv = 3\n",
    "\n",
    "# Pipe with NaN\n",
    "grid, grid_results = fit_pipe(X_train, y_train, full_pipe, scorer, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:49:23.257676Z",
     "start_time": "2019-03-28T16:49:23.208065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_preprocessor__num__imputer_n__strategy</th>\n",
       "      <th>param_preprocessor__num__scaler</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.289927</td>\n",
       "      <td>0.293017</td>\n",
       "      <td>0.120210</td>\n",
       "      <td>0.028455</td>\n",
       "      <td>mean</td>\n",
       "      <td>None</td>\n",
       "      <td>{'preprocessor__num__imputer_n__strategy': 'me...</td>\n",
       "      <td>0.025540</td>\n",
       "      <td>0.026730</td>\n",
       "      <td>0.025142</td>\n",
       "      <td>0.025804</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.117640</td>\n",
       "      <td>0.148989</td>\n",
       "      <td>0.128726</td>\n",
       "      <td>0.022261</td>\n",
       "      <td>mean</td>\n",
       "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
       "      <td>{'preprocessor__num__imputer_n__strategy': 'me...</td>\n",
       "      <td>0.019196</td>\n",
       "      <td>0.022004</td>\n",
       "      <td>0.018972</td>\n",
       "      <td>0.020057</td>\n",
       "      <td>0.001380</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.240462</td>\n",
       "      <td>0.271520</td>\n",
       "      <td>0.151623</td>\n",
       "      <td>0.056530</td>\n",
       "      <td>median</td>\n",
       "      <td>None</td>\n",
       "      <td>{'preprocessor__num__imputer_n__strategy': 'me...</td>\n",
       "      <td>0.025539</td>\n",
       "      <td>0.026729</td>\n",
       "      <td>0.025139</td>\n",
       "      <td>0.025802</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.901496</td>\n",
       "      <td>0.174241</td>\n",
       "      <td>0.069253</td>\n",
       "      <td>0.022113</td>\n",
       "      <td>median</td>\n",
       "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
       "      <td>{'preprocessor__num__imputer_n__strategy': 'me...</td>\n",
       "      <td>0.019196</td>\n",
       "      <td>0.022006</td>\n",
       "      <td>0.018972</td>\n",
       "      <td>0.020058</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       1.289927      0.293017         0.120210        0.028455   \n",
       "1       1.117640      0.148989         0.128726        0.022261   \n",
       "2       1.240462      0.271520         0.151623        0.056530   \n",
       "3       0.901496      0.174241         0.069253        0.022113   \n",
       "\n",
       "  param_preprocessor__num__imputer_n__strategy  \\\n",
       "0                                         mean   \n",
       "1                                         mean   \n",
       "2                                       median   \n",
       "3                                       median   \n",
       "\n",
       "                     param_preprocessor__num__scaler  \\\n",
       "0                                               None   \n",
       "1  StandardScaler(copy=True, with_mean=True, with...   \n",
       "2                                               None   \n",
       "3  StandardScaler(copy=True, with_mean=True, with...   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'preprocessor__num__imputer_n__strategy': 'me...           0.025540   \n",
       "1  {'preprocessor__num__imputer_n__strategy': 'me...           0.019196   \n",
       "2  {'preprocessor__num__imputer_n__strategy': 'me...           0.025539   \n",
       "3  {'preprocessor__num__imputer_n__strategy': 'me...           0.019196   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.026730           0.025142         0.025804        0.000675   \n",
       "1           0.022004           0.018972         0.020057        0.001380   \n",
       "2           0.026729           0.025139         0.025802        0.000676   \n",
       "3           0.022006           0.018972         0.020058        0.001381   \n",
       "\n",
       "   rank_test_score  \n",
       "0                1  \n",
       "1                4  \n",
       "2                2  \n",
       "3                3  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:50:14.369293Z",
     "start_time": "2019-03-28T16:50:14.352505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025804104273622258"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:55:47.489179Z",
     "start_time": "2019-03-28T16:55:47.473388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020057310582132556"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brk\n",
    "# Here I have to go for the smallest score actually\n",
    "np.min(grid_results['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:23.793014Z",
     "start_time": "2019-03-28T16:47:19.729604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    3.3s finished\n"
     ]
    }
   ],
   "source": [
    "# Pipe without NaN\n",
    "grid_reduced, grid_results_reduced = fit_pipe(X_train_reduced, y_train_reduced, \n",
    "                                              full_pipe_reduced, scorer, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:23.815936Z",
     "start_time": "2019-03-28T16:47:23.797926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1602627550424679"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(grid_reduced.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T15:03:03.900689Z",
     "start_time": "2019-03-28T15:03:03.862669Z"
    }
   },
   "source": [
    "**Result:** Results are really, really close. Imputing with Median is preferred to imputing with Mean (Grid Search), but the eliminiation of all columns with NaN scores slightly better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Outlier-Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:24.321069Z",
     "start_time": "2019-03-28T16:47:23.826336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YearRemodAdd\n",
      "Rows removed: 0\n",
      "\n",
      "FullBath\n",
      "Rows removed: 0\n",
      "\n",
      "OverallQual\n",
      "Rows removed: 1\n",
      "\n",
      "GrLivArea\n",
      "Rows removed: 7\n",
      "\n",
      "TotRmsAbvGrd\n",
      "Rows removed: 26\n",
      "\n",
      "GarageCars\n",
      "Rows removed: 4\n",
      "\n",
      "GarageArea\n",
      "Rows removed: 14\n",
      "\n",
      "TotalBsmtSF\n",
      "Rows removed: 54\n",
      "\n",
      "1stFlrSF\n",
      "Rows removed: 1\n",
      "\n",
      "YearBuilt\n",
      "Rows removed: 6\n",
      "\n",
      "SalePrice\n",
      "Rows removed: 13\n",
      "\n",
      "\n",
      "Rows removed in total: 126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove Outliers for remaining top_corr_cols in the reduced data set\n",
    "# GarageYrBlt was a the top_corr_features that was dropped above\n",
    "top_corr_columns = set(train_data_reduced.columns).intersection(set(top_corr_columns))\n",
    "train_data_outliers = cleaning.remove_outliers_IQR_method(train_data_reduced,\n",
    "                                                         top_corr_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:29.360247Z",
     "start_time": "2019-03-28T16:47:24.323553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    4.2s finished\n"
     ]
    }
   ],
   "source": [
    "X_train_outliers = train_data.drop('SalePrice', axis=1)\n",
    "y_train_outliers = train_data['SalePrice'].copy()\n",
    "\n",
    "categorical_features_outliers = X_train.select_dtypes(include=['category']).columns\n",
    "numeric_features_outliers = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Build pipeline for reduced train set without Outliers\n",
    "full_pipe_outliers = build_pipe(X_train_outliers, y_train_outliers, \n",
    "                               numeric_features_outliers, \n",
    "                               categorical_features_outliers, \n",
    "                               elastic_net_simple) # optimized model from above\n",
    "\n",
    "grid_outliers, grid_results_outliers = fit_pipe(X_train_outliers, y_train_outliers,\n",
    "                                              full_pipe_outliers, scorer, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:29.381603Z",
     "start_time": "2019-03-28T16:47:29.365771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16063655957976147"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(grid_outliers.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** Result on data with removed outliers is slightly worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Multicollinearity-Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:29.439769Z",
     "start_time": "2019-03-28T16:47:29.400878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GarageArea successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove Outliers for remaining top_corr_cols in the reduced data set\n",
    "# GarageYrBlt was a the top_corr_features that was dropped above\n",
    "cols_multi = set(train_data_reduced.columns).intersection(set(['1stFloor', 'GarageArea', 'FirstFlSF']))\n",
    "train_data_multi = cleaning.delete_columns(train_data_reduced,  cols_multi)\n",
    "assert train_data_multi.shape[1] == train_data_reduced.shape[1] - len(cols_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:34.637501Z",
     "start_time": "2019-03-28T16:47:29.443701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    4.3s finished\n"
     ]
    }
   ],
   "source": [
    "X_train_multi = train_data.drop('SalePrice', axis=1)\n",
    "y_train_multi = train_data['SalePrice'].copy()\n",
    "\n",
    "categorical_features_multi = X_train.select_dtypes(include=['category']).columns\n",
    "numeric_features_multi = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Build pipeline for reduced train set without multi\n",
    "full_pipe_multi = build_pipe(X_train_multi, y_train_multi, \n",
    "                               numeric_features_multi, \n",
    "                               categorical_features_multi, \n",
    "                               elastic_net_simple) # optimized model from above\n",
    "\n",
    "grid_multi, grid_results_multi = fit_pipe(X_train_multi, y_train_multi,\n",
    "                                              full_pipe_multi, scorer, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:34.660074Z",
     "start_time": "2019-03-28T16:47:34.640264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16063655957976147"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(grid_multi.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** Result on data with removed multi_col(s) is slightly worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:34.702888Z",
     "start_time": "2019-03-28T16:47:34.664648Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = train_data_reduced.drop(['SalePrice'], axis = 1), train_data_reduced['SalePrice']\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:34.742969Z",
     "start_time": "2019-03-28T16:47:34.710178Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assemble pipeline (define function)\n",
    "def build_pipe(X_train, y_train, numeric_features, categorical_features, clf):\n",
    "    \"\"\"Build a pipeline for preprocessing and modelling.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "        X_train: training features (df or array)\n",
    "        y_train: training labels (df or array)\n",
    "        numeric_features: list of strings, numeric columns\n",
    "        categorical_features: list of strings, categorical columns\n",
    "        clf: classifier (sk-learn model object)\n",
    "        \n",
    "    RETURNS:\n",
    "        full_pipe: pipeline object\n",
    "    \"\"\"\n",
    "    # level 1 - two separate pipes for cat and num features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer_n', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "            ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer_c', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore')),\n",
    "            ])\n",
    "\n",
    "    # level 2 - wrap the two level 1 pipes into a ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features),\n",
    "                         ])\n",
    "\n",
    "    # level 3 - pipe it with a classifier\n",
    "    full_pipe = Pipeline(steps=[\n",
    "                       ('preprocessor', preprocessor),\n",
    "                       ('clf', model_simple),\n",
    "                               ]) \n",
    "    \n",
    "    return full_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:35.288024Z",
     "start_time": "2019-03-28T16:47:34.746942Z"
    }
   },
   "outputs": [],
   "source": [
    "elastic_net_final = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], \n",
    "                                 eps=1e-3, n_alphas=100, fit_intercept=True, \n",
    "                                 normalize=True, precompute='auto', max_iter=2000, \n",
    "                                 tol=0.0001, cv=6, copy_X=True, verbose=0, n_jobs=-1, \n",
    "                                 positive=False, random_state=0)\n",
    "\n",
    "full_pipe_final = build_pipe(X_train_reduced, y_train_reduced, \n",
    "                               numeric_features_reduced, \n",
    "                               categorical_features_reduced, \n",
    "                               elastic_net_final) # cv on parameters\n",
    "\n",
    "full_pipe_final.fit(X_train_reduced, y_train_reduced)\n",
    "y_pred = full_pipe_final.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:35.326663Z",
     "start_time": "2019-03-28T16:47:35.293792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test r2 score:  0.9028418562579344\n",
      "Test RMSE: 0.1253\n"
     ]
    }
   ],
   "source": [
    "print('Test r2 score: ', r2_score(y_test, y_pred))\n",
    "test_mse = mean_squared_error(y_pred, y_test)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "print('Test RMSE: %.4f' % test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:47:35.347656Z",
     "start_time": "2019-03-28T16:47:35.331098Z"
    }
   },
   "outputs": [],
   "source": [
    "# # OLD PIPE\n",
    "\n",
    "# cols_to_crop = top_corr_columns[1:]  # 'SalePrice' has to be dropped\n",
    "# cols_to_del_multicol = ['1stFlrSF', 'GarageArea', 'TotRmsAbvGrd', 'GarageYrBlt']\n",
    "\n",
    "\n",
    "# first_transformer = Pipeline(steps=[\n",
    "#     ('crop', OutlierDropperIQR(columns=cols_to_crop)),\n",
    "# #     ('drop', ColumnDropper(columns=cols_to_del_multicol)),\n",
    "#     ])\n",
    "\n",
    "# # level 1 - two separate pipes for cat and num features\n",
    "\n",
    "# numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('scaler', StandardScaler())])\n",
    "\n",
    "# categorical_features = X_train.select_dtypes(include=['category']).columns\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# # level 2 - wrap the two level 1 pipes into a ColumnTransformer\n",
    "# preprocessor = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             ('num', numeric_transformer, numeric_features),\n",
    "#             ('cat', categorical_transformer, categorical_features),\n",
    "#                      ])\n",
    "\n",
    "# # level 3 - pipe it with a classifier\n",
    "# clf = Pipeline(steps=[\n",
    "#                    ('first', first_transformer),\n",
    "#                    ('preprocessor', preprocessor),\n",
    "#                    ('regressor', model_simple),\n",
    "#                      ]) \n",
    "\n",
    "# # apply the preprocessor and then pass transformed data to the predictor \n",
    "# clf.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "464.86px",
    "left": "1092.86px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
