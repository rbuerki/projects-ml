{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords', 'averaged_perceptron_tagger', 'maxent_ne_chunker', 'words'])\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(); color='rebeccapurple'\n",
    "%matplotlib inline  \n",
    "\n",
    "# display settings\n",
    "pd.set_option('max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database with `read_sql_table`\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql_table('messages', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test sets (with stratified sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for StratifiedShuffleSlpit to work properly all values > 10 in ['total'] column will be set to 11.\n",
    "df['total'] = np.where((df['total'] >10), 11, df['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create testset with stratified sampling according to the category count per message\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 111)\n",
    "for train_index, test_index in split.split(df, df['total']):\n",
    "    train = df.loc[train_index]\n",
    "    test = df.loc[test_index]\n",
    "    \n",
    "# safety-check\n",
    "assert len(test) == round(len(df) * 0.2), 'split got messed up'\n",
    "assert (round(df['related'].sum() / len(df),3)) == (round(train['related'].sum() / len(train),3)), 'not properly stratisfied'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'total' column from train and test sets\n",
    "for set_ in (train, test):\n",
    "    set_.drop('total', axis=1, inplace=True)\n",
    "    \n",
    "# safety-check\n",
    "assert len(test.columns) == 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into features an target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['message'].values\n",
    "y_train = train.iloc[:, 4:40].values\n",
    "\n",
    "X_test = test['message'].values\n",
    "y_test = test.iloc[:, 4:40].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ \"To determine possible evolution of the eruptive crisis of the Nyiragongo, it's necessary to improve the monitoring of the volcano, in particularly to add seismometers and to install tiltmeters to measure the ground deformation around the volcano.\",\n",
       "       'Over 25 people in line at Starbucks . People need coffee as much as they need food and gas post #Sandy ( @Starbucks ) http : //t.co/T7G3fBJg'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check\n",
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0],\n",
       "       [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check\n",
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(message):\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # normalize case and remove punctuation\n",
    "    message = re.sub(r\"[^a-zA-Z0-9]\", \" \", message.lower())\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(message)\n",
    "    # lemmatize and remove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word.strip()) for word in tokens if word not in stop_words]\n",
    "    # add part-of-speech tags\n",
    "    tokens = pos_tag(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine possible evolution of the eruptive crisis of the Nyiragongo, it's necessary to improve the monitoring of the volcano, in particularly to add seismometers and to install tiltmeters to measure the ground deformation around the volcano.\n",
      "[('determine', 'NN'), ('possible', 'JJ'), ('evolution', 'NN'), ('eruptive', 'JJ'), ('crisis', 'NN'), ('nyiragongo', 'NN'), ('necessary', 'JJ'), ('improve', 'VB'), ('monitoring', 'NN'), ('volcano', 'NN'), ('particularly', 'RB'), ('add', 'JJ'), ('seismometers', 'NNS'), ('install', 'VBP'), ('tiltmeters', 'NNS'), ('measure', 'VBP'), ('ground', 'NN'), ('deformation', 'NN'), ('around', 'IN'), ('volcano', 'NN')] \n",
      "\n",
      "Over 25 people in line at Starbucks . People need coffee as much as they need food and gas post #Sandy ( @Starbucks ) http : //t.co/T7G3fBJg\n",
      "[('25', 'CD'), ('people', 'NNS'), ('line', 'NN'), ('starbucks', 'VBZ'), ('people', 'NNS'), ('need', 'VBP'), ('coffee', 'NN'), ('much', 'JJ'), ('need', 'NN'), ('food', 'NN'), ('gas', 'NN'), ('post', 'NN'), ('sandy', 'NN'), ('starbucks', 'NNS'), ('http', 'VBP'), ('co', 'NN'), ('t7g3fbjg', 'NN')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in X_train[:2]:\n",
    "    tokens = tokenize_text(message)\n",
    "    print(message)\n",
    "    print(tokens, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "Use sk-learn's [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) ([example here](https://scikit-learn.org/stable/modules/multiclass.html#multioutput-classification))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classifier, wrapped in MultiOutputClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "\n",
    "# build the pipeline\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize_text)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(clf, n_jobs=-1, verbose=1)),\n",
    "              ])\n",
    "\n",
    "# train the pipeline\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (array([[1, 1, 1, ..., 1, 1, 1],\n       [0, 0, 1, ..., 1, 0, 1],\n       [0, 0, 0, ..., 0, 0, 0],\n       ..., \n       [1, 0, 0, ..., 0, 0, 0],\n       [0, 0, 1, ..., 0, 0, 0],\n       [1, 0, 1, ..., 1, 0, 1]]), array([[1, 1, 1, ..., 1, 1, 1],\n       [0, 0, 0, ..., 0, 0, 1],\n       [0, 0, 0, ..., 0, 0, 0],\n       ..., \n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 1]]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-c0b25f7c148b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategory_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0m_unique_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FN_UNIQUE_LABELS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_unique_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mys_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: (array([[1, 1, 1, ..., 1, 1, 1],\n       [0, 0, 1, ..., 1, 0, 1],\n       [0, 0, 0, ..., 0, 0, 0],\n       ..., \n       [1, 0, 0, ..., 0, 0, 0],\n       [0, 0, 1, ..., 0, 0, 0],\n       [1, 0, 1, ..., 1, 0, 1]]), array([[1, 1, 1, ..., 1, 1, 1],\n       [0, 0, 0, ..., 0, 0, 1],\n       [0, 0, 0, ..., 0, 0, 0],\n       ..., \n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 1]]))"
     ]
    }
   ],
   "source": [
    "category_list = train.iloc[:, 4:40].columns\n",
    "\n",
    "for col in y_test.T:\n",
    "    print(classification_report(y_test.T, y_pred.T, target_names=category_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_metrics(actual, predicted, col_names):\n",
    "    \"\"\"Calculate evaluation metrics for ML model\n",
    "    \n",
    "    Args:\n",
    "    actual: array. Array containing actual labels.\n",
    "    predicted: array. Array containing predicted labels.\n",
    "    col_names: list of strings. List containing names for each of the predicted fields.\n",
    "       \n",
    "    Returns:\n",
    "    metrics_df: dataframe. Dataframe containing the accuracy, precision, recall \n",
    "    and f1 score for a given set of actual and predicted labels.\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    # Calculate evaluation metrics for each set of labels\n",
    "    for i in range(len(col_names)):\n",
    "        accuracy = accuracy_score(actual[:, i], predicted[:, i])\n",
    "        precision = precision_score(actual[:, i], predicted[:, i])\n",
    "        recall = recall_score(actual[:, i], predicted[:, i])\n",
    "        f1 = f1_score(actual[:, i], predicted[:, i])\n",
    "        \n",
    "        metrics.append([accuracy, precision, recall, f1])\n",
    "    \n",
    "    # Create dataframe containing metrics\n",
    "    metrics = np.array(metrics)\n",
    "    metrics_df = pd.DataFrame(data = metrics, index = col_names, columns = ['Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "      \n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, Y_test, category_names):\n",
    "\n",
    "    \n",
    "\n",
    "    # predict on test data\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \n",
    "\n",
    "    accuracy = []\n",
    "\n",
    "\n",
    "\n",
    "    y_testData = pd.DataFrame(Y_test)\n",
    "\n",
    "    y_predData = pd.DataFrame(y_pred)\n",
    "\n",
    "\n",
    "\n",
    "    for col in range(len(y_testData.columns)):\n",
    "\n",
    "        accuracy.append(accuracy_score(y_testData[col],y_predData[col]))\n",
    "\n",
    " \t\t\n",
    "\n",
    "    target_colums = (category_names.iloc[:,4:].columns).tolist()\n",
    "\n",
    "\t\n",
    "\n",
    "    acc_score = pd.DataFrame(accuracy,columns=['Accuracy_score'], index=target_colums)\n",
    "\n",
    "\t\n",
    "\n",
    "    target_names = (category_names.iloc[:,4:].columns).tolist()\n",
    "\n",
    "\t\n",
    "\n",
    "    output = classification_report(Y_test, y_pred, target_names=target_names)\n",
    "\n",
    "\t\n",
    "\n",
    "    print(\"Accuracy for each category:\\n\")\n",
    "\n",
    "    print(acc_score)\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"\\n\\nPrecision,recall, f1-score and support for each category:\\n\")\n",
    "\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = \n",
    "\n",
    "cv = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to do: concatenate one hot encoded genre with text\n",
    "- eventually length and total categories to"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
