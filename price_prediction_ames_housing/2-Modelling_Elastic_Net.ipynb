{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Modelling with Elastic Net\n",
    "\n",
    "Use 'quick & dirty' custom functions from my LinRegModel class to find an optimized baseline model.\n",
    "Build an evaluation pipeline to evaluate different treatments for the training data (using that baseline model).\n",
    "Save final pipeline / model for later feature importance evaluation (in notebook 4).\n",
    "\n",
    "*Note: An issue that was not completely / elegantly solved: If I use sklearns ElasticNet() class with integrated gridsearch as regressor step inside the modelling pipeline, I don't know how get the best params out of it. So I could make a preprocessing pipeline first (--> use fit\\_transform() to return a transformed data set, see Appendix) and then use the ElasticNet() class separately on that data. But then OHE would have to be made outside the pipe, because train and test sets get different dimensions using this approach (and if I would transform the whole dataset at once I would have data leakage). Or I could evaluate Elastic Net params with regular grid search. But was to lazy in the end to do that properly ...*\n",
    "\n",
    "**Data Sources**\n",
    "\n",
    "- `data/raw/train.csv`: Training set from [kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).\n",
    "\n",
    "**Output**\n",
    "\n",
    "- `data/interim/train_opti_EN.csv`: Best feature set configuration for pipeline.\n",
    "- `models/full_pipe_final.pkl`: Best pipeline / model.\n",
    "\n",
    "\n",
    "\n",
    "**Changes**\n",
    "\n",
    "- 2019-03-22: Start notebook\n",
    "- 2019-03-29: Finish notebook, save best model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-libraries,-load-data\" data-toc-modified-id=\"Import-libraries,-load-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import libraries, load data</a></span></li><li><span><a href=\"#Go-quick-&amp;-dirty\" data-toc-modified-id=\"Go-quick-&amp;-dirty-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Go quick &amp; dirty</a></span></li><li><span><a href=\"#General-data-pre-processing-(outside-of-sklearn-pipeline)\" data-toc-modified-id=\"General-data-pre-processing-(outside-of-sklearn-pipeline)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>General data pre-processing (outside of sklearn pipeline)</a></span><ul class=\"toc-item\"><li><span><a href=\"#General-pre-processing\" data-toc-modified-id=\"General-pre-processing-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>General pre-processing</a></span></li></ul></li><li><span><a href=\"#Explore-different-feature-set-options\" data-toc-modified-id=\"Explore-different-feature-set-options-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Explore different feature set options</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-pipeline-with-CV-to-evaluate-different-options\" data-toc-modified-id=\"Define-pipeline-with-CV-to-evaluate-different-options-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Define pipeline with CV to evaluate different options</a></span></li><li><span><a href=\"#Explore-feature-sets\" data-toc-modified-id=\"Explore-feature-sets-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Explore feature sets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-train-set-(with-NaN-and-all-Outliers)\" data-toc-modified-id=\"Standard-train-set-(with-NaN-and-all-Outliers)-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Standard train set (with NaN and all Outliers)</a></span></li><li><span><a href=\"#Train-set-without-columns-containing-NaN\" data-toc-modified-id=\"Train-set-without-columns-containing-NaN-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Train set without columns containing NaN</a></span></li><li><span><a href=\"#Full-train-set-with-outliers-removed-for-top-correlating-columns\" data-toc-modified-id=\"Full-train-set-with-outliers-removed-for-top-correlating-columns-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Full train set with outliers removed for top correlating columns</a></span></li><li><span><a href=\"#Full-train-set-with-outliers-removed-and-multi-correlation-columns-removed\" data-toc-modified-id=\"Full-train-set-with-outliers-removed-and-multi-correlation-columns-removed-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>Full train set with outliers removed and multi-correlation columns removed</a></span></li></ul></li><li><span><a href=\"#Final-Tuning-&amp;-Evaluation\" data-toc-modified-id=\"Final-Tuning-&amp;-Evaluation-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Final Tuning &amp; Evaluation</a></span></li></ul></li><li><span><a href=\"#Apppendix:-Experiment-with-preprocessing-pipe-only\" data-toc-modified-id=\"Apppendix:-Experiment-with-preprocessing-pipe-only-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Apppendix: Experiment with preprocessing pipe only</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries, load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:16.103766Z",
     "start_time": "2019-04-04T13:55:13.414322Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# My functions\n",
    "import EDA_functions as EDA\n",
    "import cleaning_functions as cleaning\n",
    "from linRegModel_class import LinRegModel\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #, sns.set_style('whitegrid')\n",
    "color = 'rebeccapurple'\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:16.193567Z",
     "start_time": "2019-04-04T13:55:16.108377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "raw_data = pd.read_csv('data/raw/train.csv')\n",
    "\n",
    "# Check shape\n",
    "display(raw_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:16.224375Z",
     "start_time": "2019-04-04T13:55:16.200638Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load variables from notebook 1\n",
    "%store -r cols_to_del\n",
    "%store -r cols_to_log\n",
    "%store -r outliers_to_del\n",
    "%store -r top_corr_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go quick & dirty\n",
    "Use my 'quick & dirty' function for a baseline model on unprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:16.273309Z",
     "start_time": "2019-04-04T13:55:16.235298Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=[0.03, 0.05, 0.09], copy_X=True, cv=5, eps=0.001,\n",
       "       fit_intercept=True, l1_ratio=[0.6, 0.9, 1.0], max_iter=3000,\n",
       "       n_alphas=100, n_jobs=-1, normalize=False, positive=False,\n",
       "       precompute='auto', random_state=None, selection='cyclic',\n",
       "       tol=0.0001, verbose=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize a scikit-learn model object of choice\n",
    "model_simple = ElasticNetCV(alphas=[0.03, 0.05, 0.09], copy_X=True, cv=5, eps=0.001, \n",
    "                            fit_intercept=True, l1_ratio=[0.6, 0.9, 1.0], max_iter=3000, \n",
    "                            n_alphas=100, n_jobs=-1)\n",
    "\n",
    "# Create an instance of the LinRegModel class by passing df, target variable and model object\n",
    "elastic_net_simple = LinRegModel(raw_data, 'SalePrice', model_simple)\n",
    "\n",
    "# Output instance\n",
    "display(elastic_net_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:23.113057Z",
     "start_time": "2019-04-04T13:55:16.276819Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Perform the modelling\n",
    "elastic_net_simple.go_quickDirty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:23.145134Z",
     "start_time": "2019-04-04T13:55:23.118262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=[0.03, 0.05, 0.09], copy_X=True, cv=5, eps=0.001,\n",
       "       fit_intercept=True, l1_ratio=[0.6, 0.9, 1.0], max_iter=3000,\n",
       "       n_alphas=100, n_jobs=-1, normalize=False, positive=False,\n",
       "       precompute='auto', random_state=None, selection='cyclic',\n",
       "       tol=0.0001, verbose=0)\n",
       "\n",
       "RMSE on test data 33373.76, r2-score 0.80."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output result\n",
    "elastic_net_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:23.177372Z",
     "start_time": "2019-04-04T13:55:23.150388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09\n",
      "0.9\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "# Check best values\n",
    "print(model_simple.alpha_)\n",
    "print(model_simple.l1_ratio_)\n",
    "print(model_simple.n_iter_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General data pre-processing (outside of sklearn pipeline)\n",
    "Pre-processing steps that take place before data is pipelined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T20:48:57.218638Z",
     "start_time": "2019-03-25T20:48:57.214648Z"
    }
   },
   "source": [
    "### General pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:23.543400Z",
     "start_time": "2019-04-04T13:55:23.180363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MiscFeature successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PoolQC successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'FireplaceQu successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Alley successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Id successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Fence successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Disable warning\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# Create and clean training set with variables from the EDA notebook\n",
    "train_data = (raw_data\n",
    "              .pipe(cleaning.change_dtypes, cols_to_category=raw_data.select_dtypes(object))\n",
    "              .pipe(cleaning.delete_columns, cols_to_delete=cols_to_del)\n",
    "              .pipe(cleaning.apply_log, cols_to_transform=cols_to_log)\n",
    "             )\n",
    "\n",
    "train_data.drop(outliers_to_del, inplace=True)\n",
    "train_data.dropna(subset=['MasVnrArea', 'MasVnrType', 'Electrical'], inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:23.574552Z",
     "start_time": "2019-04-04T13:55:23.548395Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1447, 75)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check results\n",
    "display(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T22:31:35.741110Z",
     "start_time": "2019-03-22T22:31:35.737110Z"
    }
   },
   "source": [
    "## Explore different feature set options\n",
    "\n",
    "### Define pipeline with CV to evaluate different options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:23.718385Z",
     "start_time": "2019-04-04T13:55:23.578999Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_feature_sets(df, reg, scorer, cv=StratifiedKFold(3)):\n",
    "    \"\"\"Build a pipeline for evaluating different combinations of data, models\n",
    "    and scorers with stratified crossevaluation. The pipeline performs \n",
    "    necessary transformations onf categorical and numeric features and \n",
    "    evaluates the imputation strategy or if numerical data is to scale or\n",
    "    not. In this notebook my only changing variable will be the input data\n",
    "    \n",
    "    ARGUMENTS:\n",
    "        df: dataframe, input data for modelling\n",
    "        reg: sklearn model instance, a baseline model\n",
    "        scorer: string or make_score() object (?) \n",
    "        \n",
    "    RETURNS:\n",
    "        grid_results: dict, best parameters for model\n",
    "        best_score: float, highest score value - watch out if you have a loss\n",
    "            function. Then you have to search for the minimal score value\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split input features and target label\n",
    "    X_train = df.drop('SalePrice', axis=1)\n",
    "    y_train = df['SalePrice'].copy()\n",
    "    \n",
    "    # Define cat and num feature columns\n",
    "    categorical_features = X_train.select_dtypes(include=['category']).columns\n",
    "    numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "    assert len(categorical_features) + len(numeric_features) == df.shape[1] - 1\n",
    "    \n",
    "    ## Assemble pipeline (define function)\n",
    "    \n",
    "    # level 1 - two separate pipes for cat and num features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer_n', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "            ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer_c', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore')),\n",
    "            ])\n",
    "\n",
    "    # level 2 - wrap the two level 1 pipes into a ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features),\n",
    "                         ])\n",
    "\n",
    "    # level 3 - pipe it with a classifier\n",
    "    full_pipe = Pipeline(steps=[\n",
    "                       ('preprocessor', preprocessor),\n",
    "                       ('reg', model_simple),\n",
    "                               ]) \n",
    "    \n",
    "    # Evaluate imputing strategy for missing num values and scaling\n",
    "    parameters = {\n",
    "        'preprocessor__num__imputer_n__strategy': ['mean', 'median'],\n",
    "        'preprocessor__num__scaler' : [None, StandardScaler()]\n",
    "                 }\n",
    "\n",
    "    cv = GridSearchCV(full_pipe, param_grid=parameters, scoring=scorer, n_jobs=-1, iid=False,\n",
    "                      cv=cv, error_score='raise', return_train_score=False, verbose=1)\n",
    "\n",
    "    grid = cv.fit(X_train, y_train) \n",
    "    grid_results = grid.cv_results_\n",
    "\n",
    "    # Here I have to go for the smallest score (CV expects utility function\n",
    "    # and not cost function, see Hands-OnML p. 70)\n",
    "    best_score = np.sqrt(np.min(grid_results['mean_test_score']))\n",
    "    \n",
    "    return grid_results, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:23.734317Z",
     "start_time": "2019-04-04T13:55:23.721378Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define input parameters\n",
    "scorer = make_scorer(mean_squared_error)\n",
    "reg = elastic_net_simple # 'optimized baseline model'\n",
    "cv = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T20:56:26.299332Z",
     "start_time": "2019-03-25T20:56:26.295343Z"
    }
   },
   "source": [
    "### Explore feature sets\n",
    "\n",
    "#### Standard train set (with NaN and all Outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:34.093103Z",
     "start_time": "2019-04-04T13:55:23.737310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    9.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13817449507839052\n"
     ]
    }
   ],
   "source": [
    "# Run pipeline\n",
    "grid_results, best_score = evaluate_feature_sets(train_data, reg=reg, scorer=scorer, cv=cv)\n",
    "\n",
    "# Print best score\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:34.189621Z",
     "start_time": "2019-04-04T13:55:34.099946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_preprocessor__num__imputer_n__strategy</th>\n",
       "      <th>param_preprocessor__num__scaler</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.362899</td>\n",
       "      <td>0.231008</td>\n",
       "      <td>0.080529</td>\n",
       "      <td>0.013988</td>\n",
       "      <td>mean</td>\n",
       "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
       "      <td>{'preprocessor__num__imputer_n__strategy': 'me...</td>\n",
       "      <td>0.018117</td>\n",
       "      <td>0.020913</td>\n",
       "      <td>0.018247</td>\n",
       "      <td>0.019092</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "1       1.362899      0.231008         0.080529        0.013988   \n",
       "\n",
       "  param_preprocessor__num__imputer_n__strategy  \\\n",
       "1                                         mean   \n",
       "\n",
       "                     param_preprocessor__num__scaler  \\\n",
       "1  StandardScaler(copy=True, with_mean=True, with...   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "1  {'preprocessor__num__imputer_n__strategy': 'me...           0.018117   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "1           0.020913           0.018247         0.019092        0.001289   \n",
       "\n",
       "   rank_test_score  \n",
       "1                4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(grid_results).nsmallest(1, 'mean_test_score'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** Best score for imputation with mean and applied StandardScaler(). (The latter has an impact but imputation with 'mean' or 'median' leads to more or less the same result.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T20:56:26.299332Z",
     "start_time": "2019-03-25T20:56:26.295343Z"
    }
   },
   "source": [
    "#### Train set without columns containing NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:34.253292Z",
     "start_time": "2019-04-04T13:55:34.193719Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create List of Columns containing NaN\n",
    "nan_cols = []\n",
    "for col in train_data.columns:\n",
    "    if train_data[col].isnull().sum() > 0:\n",
    "        nan_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:34.273076Z",
     "start_time": "2019-04-04T13:55:34.254692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LotFrontage',\n",
       " 'BsmtQual',\n",
       " 'BsmtCond',\n",
       " 'BsmtExposure',\n",
       " 'BsmtFinType1',\n",
       " 'BsmtFinType2',\n",
       " 'GarageType',\n",
       " 'GarageYrBlt',\n",
       " 'GarageFinish',\n",
       " 'GarageQual',\n",
       " 'GarageCond']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check results\n",
    "nan_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:34.303687Z",
     "start_time": "2019-04-04T13:55:34.275196Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create train set without missing values (drop nan_cols)\n",
    "train_data_reduced = train_data.drop(nan_cols, axis=1)\n",
    "\n",
    "assert train_data_reduced.isnull().sum().sum() == 0\n",
    "assert train_data_reduced.shape[1] == train_data.shape[1] - len(nan_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:38.872692Z",
     "start_time": "2019-04-04T13:55:34.307063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    3.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13817450160297168\n"
     ]
    }
   ],
   "source": [
    "# Run pipeline\n",
    "grid_results, best_score = evaluate_feature_sets(train_data_reduced, \n",
    "                                                 reg=reg, scorer=scorer, cv=cv)\n",
    "\n",
    "# Print best score\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T15:03:03.900689Z",
     "start_time": "2019-03-28T15:03:03.862669Z"
    }
   },
   "source": [
    "**Result:** Results are really, really close. Imputing with Mean scores slightly better than elimination of the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full train set with outliers removed for top correlating columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:39.542534Z",
     "start_time": "2019-04-04T13:55:38.883298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotalBsmtSF\n",
      "Rows removed: 49\n",
      "\n",
      "GarageCars\n",
      "Rows removed: 5\n",
      "\n",
      "GrLivArea\n",
      "Rows removed: 7\n",
      "\n",
      "GarageArea\n",
      "Rows removed: 18\n",
      "\n",
      "YearRemodAdd\n",
      "Rows removed: 0\n",
      "\n",
      "SalePrice\n",
      "Rows removed: 17\n",
      "\n",
      "1stFlrSF\n",
      "Rows removed: 1\n",
      "\n",
      "YearBuilt\n",
      "Rows removed: 5\n",
      "\n",
      "TotRmsAbvGrd\n",
      "Rows removed: 16\n",
      "\n",
      "FullBath\n",
      "Rows removed: 0\n",
      "\n",
      "GarageYrBlt\n",
      "Rows removed: 1\n",
      "\n",
      "OverallQual\n",
      "Rows removed: 0\n",
      "\n",
      "\n",
      "Rows removed in total: 119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove Outliers for remaining top_corr_cols\n",
    "top_corr_columns = set(train_data.columns).intersection(set(top_corr_columns))\n",
    "train_data_outliers = cleaning.remove_outliers_IQR_method(train_data, top_corr_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:44.909243Z",
     "start_time": "2019-04-04T13:55:39.545315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    4.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12656302335354497\n"
     ]
    }
   ],
   "source": [
    "# Run pipeline\n",
    "grid_results, best_score = evaluate_feature_sets(train_data_outliers, \n",
    "                                                 reg=reg, scorer=scorer, cv=cv)\n",
    "\n",
    "# Print best score\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** Result on data with removed outliers leads to a better score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:44.958291Z",
     "start_time": "2019-04-04T13:55:44.918513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_preprocessor__num__imputer_n__strategy</th>\n",
       "      <th>param_preprocessor__num__scaler</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.172863</td>\n",
       "      <td>0.108654</td>\n",
       "      <td>0.053246</td>\n",
       "      <td>0.022438</td>\n",
       "      <td>median</td>\n",
       "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
       "      <td>{'preprocessor__num__imputer_n__strategy': 'me...</td>\n",
       "      <td>0.016128</td>\n",
       "      <td>0.015421</td>\n",
       "      <td>0.016506</td>\n",
       "      <td>0.016018</td>\n",
       "      <td>0.00045</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "3       1.172863      0.108654         0.053246        0.022438   \n",
       "\n",
       "  param_preprocessor__num__imputer_n__strategy  \\\n",
       "3                                       median   \n",
       "\n",
       "                     param_preprocessor__num__scaler  \\\n",
       "3  StandardScaler(copy=True, with_mean=True, with...   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "3  {'preprocessor__num__imputer_n__strategy': 'me...           0.016128   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "3           0.015421           0.016506         0.016018         0.00045   \n",
       "\n",
       "   rank_test_score  \n",
       "3                4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check params\n",
    "display(pd.DataFrame(grid_results).nsmallest(1, 'mean_test_score'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full train set with outliers removed and multi-correlation columns removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:44.990186Z",
     "start_time": "2019-04-04T13:55:44.961127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GarageArea successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove Outliers for remaining top_corr_cols in the reduced data set\n",
    "# GarageYrBlt was a the top_corr_features that was dropped above\n",
    "cols_multi = set(train_data_outliers.columns).intersection(set(['1stFloor', 'GarageArea', 'FirstFlSF']))\n",
    "train_data_multi = cleaning.delete_columns(train_data_outliers,  cols_multi)\n",
    "\n",
    "assert train_data_multi.shape[1] == train_data_outliers.shape[1] - len(cols_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:49.720332Z",
     "start_time": "2019-04-04T13:55:44.994344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12658308430203355\n"
     ]
    }
   ],
   "source": [
    "# Run pipeline\n",
    "grid_results, best_score = evaluate_feature_sets(train_data_multi, \n",
    "                                                 reg=reg, scorer=scorer, cv=cv)\n",
    "\n",
    "# Print best score\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** Result on data with removed multi_col(s) is slightly worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Tuning & Evaluation\n",
    "\n",
    "The full train_set with mean imputation, scaling and outlier removal scored the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:49.773067Z",
     "start_time": "2019-04-04T13:55:49.724394Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = train_data_outliers.drop(['SalePrice'], axis = 1), train_data_outliers['SalePrice']\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:55:49.839113Z",
     "start_time": "2019-04-04T13:55:49.777839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assemble pipeline (define function)\n",
    "def build_final_pipe(X_train, y_train, reg):\n",
    "    \"\"\"Build a pipeline for preprocessing and modelling.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "        X_train: training features (df or array)\n",
    "        y_train: training labels (df or array)\n",
    "        reg: classifier (sk-learn model object)\n",
    "        \n",
    "    RETURNS:\n",
    "        full_pipe: pipeline object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define cat and num feature columns\n",
    "    categorical_features = X_train.select_dtypes(include=['category']).columns\n",
    "    numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "    assert len(categorical_features) + len(numeric_features) == X_train.shape[1]\n",
    "    \n",
    "    # level 1 - two separate pipes for cat and num features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer_n', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),\n",
    "            ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer_c', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore')),\n",
    "            ])\n",
    "\n",
    "    # level 2 - wrap the two level 1 pipes into a ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features),\n",
    "                         ])\n",
    "\n",
    "    # level 3 - pipe it with a classifier\n",
    "    full_pipe = Pipeline(steps=[\n",
    "                       ('preprocessor', preprocessor),\n",
    "                       ('reg', reg),\n",
    "                               ]) \n",
    "    \n",
    "    return full_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:56:01.853403Z",
     "start_time": "2019-04-04T13:55:49.841570Z"
    }
   },
   "outputs": [],
   "source": [
    "elastic_net_final = ElasticNetCV(l1_ratio=.9, \n",
    "                                 eps=1e-3, n_alphas=100, fit_intercept=True, \n",
    "                                 normalize=True, precompute='auto', max_iter=3000, \n",
    "                                 tol=0.0001, cv=6, copy_X=True, verbose=0, n_jobs=-1, \n",
    "                                 positive=False, random_state=0)\n",
    "\n",
    "full_pipe_final = build_final_pipe(X_train, y_train, elastic_net_final) # cv on parameters\n",
    "full_pipe_final.fit(X_train, y_train)\n",
    "\n",
    "y_pred = full_pipe_final.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:56:01.883896Z",
     "start_time": "2019-04-04T13:56:01.856700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test r2 score:  0.9363510677997943\n",
      "Test RMSE: 0.0887\n"
     ]
    }
   ],
   "source": [
    "print('Test r2 score: ', r2_score(y_test, y_pred))\n",
    "test_mse = mean_squared_error(y_pred, y_test)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "print('Test RMSE: %.4f' % test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:56:26.908580Z",
     "start_time": "2019-04-04T13:56:25.675820Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save final model and data\n",
    "joblib.dump(full_pipe_final, 'models/elastic_net_final.pkl')\n",
    "train_data_outliers.to_csv('data/interim/train_opti_EN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apppendix: Experiment with preprocessing pipe only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:56:29.273329Z",
     "start_time": "2019-04-04T13:56:29.244373Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = train_data_outliers.drop(['SalePrice'], axis = 1), train_data_outliers['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:56:29.792733Z",
     "start_time": "2019-04-04T13:56:29.769238Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assemble pipeline (define function)\n",
    "def build_preprocessing_pipe(X):\n",
    "    \"\"\"Build a pipeline for preprocessing and modelling.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "        X_train: training features (df or array)\n",
    "        \n",
    "    RETURNS:\n",
    "        preprocessor: pipeline object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define cat and num feature columns\n",
    "    categorical_features = X_train.select_dtypes(include=['category']).columns\n",
    "    numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "    assert len(categorical_features) + len(numeric_features) == X_train.shape[1]\n",
    "    \n",
    "    # level 1 - two separate pipes for cat and num features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer_n', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),\n",
    "            ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer_c', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore')),\n",
    "            ])\n",
    "\n",
    "    # level 2 - wrap the two level 1 pipes into a ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features),\n",
    "                         ])\n",
    "    \n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-04T13:56:30.798255Z",
     "start_time": "2019-04-04T13:56:30.633213Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = build_preprocessing_pipe(X)\n",
    "X_tr = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "464.86px",
    "left": "1092.86px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
