{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Modelling with Elastic Net\n",
    "Use 'quick & dirty' custom functions from my LinRegModel class to find an optimized baseline model. Build an evaluation pipeline to evaluate different treatments for the training data (using that baseline model). Save final pipeline / model for later feature importance evaluation (in notebook 4).\n",
    "\n",
    "*Note: An issue that was not completely / elegantly solved: I use sklearns ElasticNetCV() class with integrated gridsearch for tuning of the baseline model. When I use it inside a pipeline I don't know how get the best params out of it. So I used ElasticNet() for the final model, but it did not work well with the params I had tuned for the baseline model ... Don't know why. I should have evaluated the final ElasticNet params with proper grid search in the pipeline, but was to lazy in the end to do that ... did some manual trial and error.*\n",
    "\n",
    "**Data Sources**\n",
    "- `data/raw/train.csv`: Training set from kaggle.\n",
    "\n",
    "**Output**\n",
    "- `feature_names`: List of column labels for preprocessed data.\n",
    "- `models/full_pipe_final.pkl`: Best pipeline / model.\n",
    "\n",
    "**Changes**\n",
    "- 2019-03-22: Start notebook\n",
    "- 2019-03-29: Finish notebook, save best model\n",
    "- 2019-04-06: Big refactoring, OHE outside final Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-libraries,-load-data\" data-toc-modified-id=\"Import-libraries,-load-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import libraries, load data</a></span></li><li><span><a href=\"#Go-quick-&amp;-dirty\" data-toc-modified-id=\"Go-quick-&amp;-dirty-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Go quick &amp; dirty</a></span></li><li><span><a href=\"#General-data-pre-processing-(outside-of-sklearn-pipeline)\" data-toc-modified-id=\"General-data-pre-processing-(outside-of-sklearn-pipeline)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>General data pre-processing (outside of sklearn pipeline)</a></span><ul class=\"toc-item\"><li><span><a href=\"#General-pre-processing\" data-toc-modified-id=\"General-pre-processing-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>General pre-processing</a></span></li></ul></li><li><span><a href=\"#Explore-different-feature-set-options\" data-toc-modified-id=\"Explore-different-feature-set-options-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Explore different feature set options</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-pipeline-with-CV-to-evaluate-different-options\" data-toc-modified-id=\"Define-pipeline-with-CV-to-evaluate-different-options-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Define pipeline with CV to evaluate different options</a></span></li><li><span><a href=\"#Explore-feature-sets\" data-toc-modified-id=\"Explore-feature-sets-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Explore feature sets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-train-set-(with-NaN-and-all-Outliers)\" data-toc-modified-id=\"Standard-train-set-(with-NaN-and-all-Outliers)-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Standard train set (with NaN and all Outliers)</a></span></li><li><span><a href=\"#Train-set-without-columns-containing-NaN\" data-toc-modified-id=\"Train-set-without-columns-containing-NaN-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Train set without columns containing NaN</a></span></li><li><span><a href=\"#Full-train-set-with-outliers-removed-for-top-correlating-columns\" data-toc-modified-id=\"Full-train-set-with-outliers-removed-for-top-correlating-columns-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Full train set with outliers removed for top correlating columns</a></span></li><li><span><a href=\"#Full-train-set-with-outliers-removed-and-multi-correlation-columns-removed\" data-toc-modified-id=\"Full-train-set-with-outliers-removed-and-multi-correlation-columns-removed-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>Full train set with outliers removed and multi-correlation columns removed</a></span></li></ul></li></ul></li><li><span><a href=\"#Final-tuning-&amp;-evaluation\" data-toc-modified-id=\"Final-tuning-&amp;-evaluation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Final tuning &amp; evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Save-final-data-and-model\" data-toc-modified-id=\"Save-final-data-and-model-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Save final data and model</a></span></li></ul></li><li><span><a href=\"#Apppendix:-Experiment-with-preprocessing-pipe-only\" data-toc-modified-id=\"Apppendix:-Experiment-with-preprocessing-pipe-only-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Apppendix: Experiment with preprocessing pipe only</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries, load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:35:41.142148Z",
     "start_time": "2019-04-06T13:35:38.898349Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# My functions\n",
    "import EDA_functions as EDA\n",
    "import cleaning_functions as cleaning\n",
    "from linRegModel_class import LinRegModel\n",
    "import custom_transformers as transform\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #, sns.set_style('whitegrid')\n",
    "color = 'rebeccapurple'\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:35:41.234791Z",
     "start_time": "2019-04-06T13:35:41.146469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "raw_data = pd.read_csv('data/raw/train.csv')\n",
    "\n",
    "# Check shape\n",
    "display(raw_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:35:41.273365Z",
     "start_time": "2019-04-06T13:35:41.234791Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load variables from notebook 1\n",
    "%store -r cols_to_del\n",
    "%store -r cols_to_log\n",
    "%store -r outliers_to_del\n",
    "%store -r top_corr_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go quick & dirty\n",
    "Use my 'quick & dirty' function for a baseline model on unprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:35:41.309640Z",
     "start_time": "2019-04-06T13:35:41.285266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=[0.03, 0.05, 0.09], copy_X=True, cv=5, eps=0.001,\n",
       "       fit_intercept=True, l1_ratio=[0.6, 0.9, 1.0], max_iter=3000,\n",
       "       n_alphas=100, n_jobs=-1, normalize=False, positive=False,\n",
       "       precompute='auto', random_state=None, selection='cyclic',\n",
       "       tol=0.0001, verbose=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize a scikit-learn model object of choice - here ElasticNetCV for some param tuning\n",
    "model_simple = ElasticNetCV(alphas=[0.03, 0.05, 0.09], copy_X=True, cv=5, eps=0.001, \n",
    "                            fit_intercept=True, l1_ratio=[0.6, 0.9, 1.0], max_iter=3000, \n",
    "                            n_alphas=100, n_jobs=-1)\n",
    "\n",
    "# Create an instance of the LinRegModel class by passing df, target variable and model object\n",
    "elastic_net_simple = LinRegModel(raw_data, 'SalePrice', model_simple)\n",
    "\n",
    "# Output instance\n",
    "display(elastic_net_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:35:48.025521Z",
     "start_time": "2019-04-06T13:35:41.312759Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\r2d4\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Perform the modelling\n",
    "elastic_net_simple.go_quickDirty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:35:48.060693Z",
     "start_time": "2019-04-06T13:35:48.031416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=[0.03, 0.05, 0.09], copy_X=True, cv=5, eps=0.001,\n",
       "       fit_intercept=True, l1_ratio=[0.6, 0.9, 1.0], max_iter=3000,\n",
       "       n_alphas=100, n_jobs=-1, normalize=False, positive=False,\n",
       "       precompute='auto', random_state=None, selection='cyclic',\n",
       "       tol=0.0001, verbose=0)\n",
       "\n",
       "RMSE on test data 33373.76, r2-score 0.80."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output result\n",
    "elastic_net_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:35:48.093503Z",
     "start_time": "2019-04-06T13:35:48.065027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09\n",
      "0.9\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "# Check best values\n",
    "print(model_simple.alpha_)\n",
    "print(model_simple.l1_ratio_)\n",
    "print(model_simple.n_iter_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General data pre-processing (outside of sklearn pipeline)\n",
    "Pre-processing steps that take place before data is pipelined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T20:48:57.218638Z",
     "start_time": "2019-03-25T20:48:57.214648Z"
    }
   },
   "source": [
    "### General pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:35:48.394889Z",
     "start_time": "2019-04-06T13:35:48.096808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MiscFeature successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PoolQC successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'FireplaceQu successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Alley successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Id successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Fence successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Disable warning\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# Create and clean training set with variables from the EDA notebook\n",
    "train_data = (raw_data\n",
    "              .pipe(cleaning.change_dtypes, cols_to_category=raw_data.select_dtypes(object))\n",
    "              .pipe(cleaning.delete_columns, cols_to_delete=cols_to_del)\n",
    "              .pipe(cleaning.apply_log, cols_to_transform=cols_to_log)\n",
    "             )\n",
    "\n",
    "train_data.drop(outliers_to_del, inplace=True)\n",
    "train_data.dropna(subset=['MasVnrArea', 'MasVnrType', 'Electrical'], inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:35:48.433835Z",
     "start_time": "2019-04-06T13:35:48.395886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1447, 75)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check results\n",
    "display(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T22:31:35.741110Z",
     "start_time": "2019-03-22T22:31:35.737110Z"
    }
   },
   "source": [
    "## Explore different feature set options\n",
    "\n",
    "### Define pipeline with CV to evaluate different options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:35:48.626556Z",
     "start_time": "2019-04-06T13:35:48.440123Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_feature_sets(df, reg, scorer, cv=StratifiedKFold(3)):\n",
    "    \"\"\"Build a pipeline for evaluating different combinations of data, models\n",
    "    and scorers with stratified crossevaluation. The pipeline performs \n",
    "    necessary transformations onf categorical and numeric features and \n",
    "    evaluates the imputation strategy or if numerical data is to scale or\n",
    "    not. In this notebook my only changing variable will be the input data\n",
    "    \n",
    "    ARGUMENTS:\n",
    "        df: dataframe, input data for modelling\n",
    "        reg: sklearn model instance, a baseline model\n",
    "        scorer: string or make_score() object (?) \n",
    "        \n",
    "    RETURNS:\n",
    "        grid_results: dict, best parameters for model\n",
    "        best_score: float, highest score value - watch out if you have a loss\n",
    "            function. Then you have to search for the minimal score value\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split input features and target label\n",
    "    X_train = df.drop('SalePrice', axis=1)\n",
    "    y_train = df['SalePrice'].copy()\n",
    "    \n",
    "    # Define cat and num feature columns\n",
    "    categorical_features = X_train.select_dtypes(include=['category']).columns\n",
    "    numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "    assert len(categorical_features) + len(numeric_features) == df.shape[1] - 1\n",
    "    \n",
    "    ## Assemble pipeline (define function)\n",
    "    \n",
    "    # level 1 - two separate pipes for cat and num features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer_n', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "            ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer_c', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore')),\n",
    "            ])\n",
    "\n",
    "    # level 2 - wrap the two level 1 pipes into a ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features),\n",
    "                         ])\n",
    "\n",
    "    # level 3 - pipe it with a classifier\n",
    "    full_pipe = Pipeline(steps=[\n",
    "                       ('preprocessor', preprocessor),\n",
    "                       ('reg', model_simple),\n",
    "                               ]) \n",
    "    \n",
    "    # Evaluate imputing strategy for missing num values and scaling\n",
    "    parameters = {\n",
    "        'preprocessor__num__imputer_n__strategy': ['mean', 'median'],\n",
    "        'preprocessor__num__scaler' : [None, StandardScaler()]\n",
    "                 }\n",
    "\n",
    "    cv = GridSearchCV(full_pipe, param_grid=parameters, scoring=scorer, n_jobs=-1, iid=False,\n",
    "                      cv=cv, error_score='raise', return_train_score=False, verbose=1)\n",
    "\n",
    "    grid = cv.fit(X_train, y_train) \n",
    "    grid_results = grid.cv_results_\n",
    "\n",
    "    # Here I have to go for the smallest score (CV expects utility function\n",
    "    # and not cost function, see Hands-OnML p. 70)\n",
    "    best_score = np.sqrt(np.min(grid_results['mean_test_score']))\n",
    "    \n",
    "    return grid_results, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:35:48.642267Z",
     "start_time": "2019-04-06T13:35:48.627554Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define input parameters\n",
    "scorer = make_scorer(mean_squared_error)\n",
    "reg = elastic_net_simple # 'optimized baseline model'\n",
    "cv = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T20:56:26.299332Z",
     "start_time": "2019-03-25T20:56:26.295343Z"
    }
   },
   "source": [
    "### Explore feature sets\n",
    "\n",
    "#### Standard train set (with NaN and all Outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:02.643148Z",
     "start_time": "2019-04-06T13:35:48.645066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:   12.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13817449507839052\n"
     ]
    }
   ],
   "source": [
    "# Run pipeline\n",
    "grid_results, best_score = evaluate_feature_sets(train_data, reg=reg, scorer=scorer, cv=cv)\n",
    "\n",
    "# Print best score\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:02.709294Z",
     "start_time": "2019-04-06T13:36:02.644149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_preprocessor__num__imputer_n__strategy</th>\n",
       "      <th>param_preprocessor__num__scaler</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.568139</td>\n",
       "      <td>0.436887</td>\n",
       "      <td>0.089018</td>\n",
       "      <td>0.01974</td>\n",
       "      <td>mean</td>\n",
       "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
       "      <td>{'preprocessor__num__imputer_n__strategy': 'me...</td>\n",
       "      <td>0.018117</td>\n",
       "      <td>0.020913</td>\n",
       "      <td>0.018247</td>\n",
       "      <td>0.019092</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "1       1.568139      0.436887         0.089018         0.01974   \n",
       "\n",
       "  param_preprocessor__num__imputer_n__strategy  \\\n",
       "1                                         mean   \n",
       "\n",
       "                     param_preprocessor__num__scaler  \\\n",
       "1  StandardScaler(copy=True, with_mean=True, with...   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "1  {'preprocessor__num__imputer_n__strategy': 'me...           0.018117   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "1           0.020913           0.018247         0.019092        0.001289   \n",
       "\n",
       "   rank_test_score  \n",
       "1                4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(grid_results).nsmallest(1, 'mean_test_score'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** Best score for imputation with mean and applied StandardScaler(). (The latter has an impact but imputation with 'mean' or 'median' leads to more or less the same result.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T20:56:26.299332Z",
     "start_time": "2019-03-25T20:56:26.295343Z"
    }
   },
   "source": [
    "#### Train set without columns containing NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:02.825124Z",
     "start_time": "2019-04-06T13:36:02.723001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create List of Columns containing NaN\n",
    "nan_cols = []\n",
    "for col in train_data.columns:\n",
    "    if train_data[col].isnull().sum() > 0:\n",
    "        nan_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:02.840471Z",
     "start_time": "2019-04-06T13:36:02.830706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LotFrontage',\n",
       " 'BsmtQual',\n",
       " 'BsmtCond',\n",
       " 'BsmtExposure',\n",
       " 'BsmtFinType1',\n",
       " 'BsmtFinType2',\n",
       " 'GarageType',\n",
       " 'GarageYrBlt',\n",
       " 'GarageFinish',\n",
       " 'GarageQual',\n",
       " 'GarageCond']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check results\n",
    "nan_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:02.873013Z",
     "start_time": "2019-04-06T13:36:02.845925Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create train set without missing values (drop nan_cols)\n",
    "train_data_reduced = train_data.drop(nan_cols, axis=1)\n",
    "\n",
    "assert train_data_reduced.isnull().sum().sum() == 0\n",
    "assert train_data_reduced.shape[1] == train_data.shape[1] - len(nan_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:07.611083Z",
     "start_time": "2019-04-06T13:36:02.881631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    4.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13817450160297168\n"
     ]
    }
   ],
   "source": [
    "# Run pipeline\n",
    "grid_results, best_score = evaluate_feature_sets(train_data_reduced, \n",
    "                                                 reg=reg, scorer=scorer, cv=cv)\n",
    "\n",
    "# Print best score\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T15:03:03.900689Z",
     "start_time": "2019-03-28T15:03:03.862669Z"
    }
   },
   "source": [
    "**Result:** Results are really, really close. Imputing with Mean scores slightly better than elimination of the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full train set with outliers removed for top correlating columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:08.183241Z",
     "start_time": "2019-04-06T13:36:07.612080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullBath\n",
      "Rows removed: 0\n",
      "\n",
      "YearRemodAdd\n",
      "Rows removed: 0\n",
      "\n",
      "YearBuilt\n",
      "Rows removed: 7\n",
      "\n",
      "OverallQual\n",
      "Rows removed: 1\n",
      "\n",
      "TotalBsmtSF\n",
      "Rows removed: 49\n",
      "\n",
      "GarageArea\n",
      "Rows removed: 20\n",
      "\n",
      "SalePrice\n",
      "Rows removed: 21\n",
      "\n",
      "1stFlrSF\n",
      "Rows removed: 1\n",
      "\n",
      "GrLivArea\n",
      "Rows removed: 1\n",
      "\n",
      "TotRmsAbvGrd\n",
      "Rows removed: 16\n",
      "\n",
      "GarageCars\n",
      "Rows removed: 2\n",
      "\n",
      "GarageYrBlt\n",
      "Rows removed: 1\n",
      "\n",
      "\n",
      "Rows removed in total: 119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove Outliers for remaining top_corr_cols\n",
    "top_corr_columns = set(train_data.columns).intersection(set(top_corr_columns))\n",
    "train_data_outliers = cleaning.remove_outliers_IQR_method(train_data, top_corr_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:13.978566Z",
     "start_time": "2019-04-06T13:36:08.186939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    4.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12656302335354497\n"
     ]
    }
   ],
   "source": [
    "# Run pipeline\n",
    "grid_results, best_score = evaluate_feature_sets(train_data_outliers, \n",
    "                                                 reg=reg, scorer=scorer, cv=cv)\n",
    "\n",
    "# Print best score\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** Result on data with removed outliers leads to a better score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:14.052817Z",
     "start_time": "2019-04-06T13:36:13.983395Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_preprocessor__num__imputer_n__strategy</th>\n",
       "      <th>param_preprocessor__num__scaler</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.45768</td>\n",
       "      <td>0.246447</td>\n",
       "      <td>0.036368</td>\n",
       "      <td>0.005242</td>\n",
       "      <td>median</td>\n",
       "      <td>StandardScaler(copy=True, with_mean=True, with...</td>\n",
       "      <td>{'preprocessor__num__imputer_n__strategy': 'me...</td>\n",
       "      <td>0.016128</td>\n",
       "      <td>0.015421</td>\n",
       "      <td>0.016506</td>\n",
       "      <td>0.016018</td>\n",
       "      <td>0.00045</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "3        1.45768      0.246447         0.036368        0.005242   \n",
       "\n",
       "  param_preprocessor__num__imputer_n__strategy  \\\n",
       "3                                       median   \n",
       "\n",
       "                     param_preprocessor__num__scaler  \\\n",
       "3  StandardScaler(copy=True, with_mean=True, with...   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "3  {'preprocessor__num__imputer_n__strategy': 'me...           0.016128   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "3           0.015421           0.016506         0.016018         0.00045   \n",
       "\n",
       "   rank_test_score  \n",
       "3                4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check params\n",
    "display(pd.DataFrame(grid_results).nsmallest(1, 'mean_test_score'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full train set with outliers removed and multi-correlation columns removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:14.100451Z",
     "start_time": "2019-04-06T13:36:14.053837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GarageArea successfully deleted'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove Outliers for remaining top_corr_cols in the reduced data set\n",
    "# GarageYrBlt was a the top_corr_features that was dropped above\n",
    "cols_multi = set(train_data_outliers.columns).intersection(set(['1stFloor', 'GarageArea', 'FirstFlSF']))\n",
    "train_data_multi = cleaning.delete_columns(train_data_outliers,  cols_multi)\n",
    "\n",
    "assert train_data_multi.shape[1] == train_data_outliers.shape[1] - len(cols_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:19.517098Z",
     "start_time": "2019-04-06T13:36:14.104386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    4.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12658308430203355\n"
     ]
    }
   ],
   "source": [
    "# Run pipeline\n",
    "grid_results, best_score = evaluate_feature_sets(train_data_multi, \n",
    "                                                 reg=reg, scorer=scorer, cv=cv)\n",
    "\n",
    "# Print best score\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** Result on data with removed multi_col(s) is slightly worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T12:17:48.435226Z",
     "start_time": "2019-04-06T12:17:48.429610Z"
    }
   },
   "source": [
    "## Final tuning & evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full train_set with mean imputation, scaling and outlier removal scored the best results. So I will use this config. I will do the OHE outside the pipeline so I have no problems afterwards, when I want to inspect feature importance. I will also tune the regressor inside the pipeline this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:19.770132Z",
     "start_time": "2019-04-06T13:36:19.521984Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split features and target variable\n",
    "X_temp = train_data_outliers.drop(['SalePrice'], axis = 1)\n",
    "y = train_data_outliers['SalePrice'].copy()\n",
    "# One-Hot-Encode features and save column names\n",
    "X = pd.get_dummies(X_temp, dummy_na=True)\n",
    "feature_names = X.columns\n",
    "# Save names for cat and num features separately (for pipeline)\n",
    "num_features = list(X_temp.select_dtypes(include=['float64', 'int64']).columns)\n",
    "cat_features = list(set(feature_names).difference(set(num_features)))\n",
    "assert len(num_features) + len(cat_features) == len(feature_names)\n",
    "                        \n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size = 0.2, random_state = 666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:19.845890Z",
     "start_time": "2019-04-06T13:36:19.773728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond'], dtype='object')\n",
      "Number of one-hot-encoded features:  308\n"
     ]
    }
   ],
   "source": [
    "# Check feature_names\n",
    "print(feature_names[:5])\n",
    "print(\"Number of one-hot-encoded features: \", len(feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T12:27:57.171828Z",
     "start_time": "2019-04-06T12:27:57.156938Z"
    }
   },
   "source": [
    "Note: Numeric features are passed before categorical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:19.924235Z",
     "start_time": "2019-04-06T13:36:19.851323Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assemble pipeline (define function)\n",
    "def build_final_pipe(X_train, y_train, reg):\n",
    "    \"\"\"Build a pipeline for preprocessing and modelling.\n",
    "    \n",
    "    ARGUMENTS:\n",
    "        X_train: training features (df or array)\n",
    "        y_train: training labels (df or array)\n",
    "        reg: classifier (sk-learn model object)\n",
    "        \n",
    "    RETURNS:\n",
    "        full_pipe: pipeline object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define cat and num feature columns\n",
    "    categorical_features = X_train[cat_features].columns\n",
    "    numeric_features = X_train[num_features].columns\n",
    "    assert len(categorical_features) + len(numeric_features) == X_train.shape[1]\n",
    "    \n",
    "    # level 1 - two separate pipes for cat and num features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer_n', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),\n",
    "            ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('pass', transform.PassthroughTransformer()), # Simple Passtrough\n",
    "            ])\n",
    "\n",
    "    # level 2 - wrap the two level 1 pipes into a ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features),\n",
    "                         ])\n",
    "\n",
    "    # level 3 - pipe it with a classifier\n",
    "    full_pipe = Pipeline(steps=[\n",
    "                       ('preprocessor', preprocessor),\n",
    "                       ('reg', reg),\n",
    "                               ]) \n",
    "    \n",
    "    return preprocessor, full_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:20.054595Z",
     "start_time": "2019-04-06T13:36:19.929688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define final model withouth CV, some parameters changed. \n",
    "# Among others I lowered l1_ratio to have more stability (trading in some precision)\n",
    "elastic_net_final = ElasticNet(alpha=0.0009, l1_ratio=0.5, max_iter=3000,\n",
    "                               fit_intercept=True, normalize=False, precompute=False, \n",
    "                                 tol=0.0001, copy_X=True, warm_start=False, \n",
    "                                 positive=False, random_state=666)\n",
    "\n",
    "# Build, fit, predict\n",
    "preprocessor_final, full_pipe_final = build_final_pipe(X_train, y_train, elastic_net_final) \n",
    "full_pipe_final.fit(X_train, y_train)\n",
    "y_pred = full_pipe_final.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:20.085847Z",
     "start_time": "2019-04-06T13:36:20.060857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test r2 score:  0.9232344582903343\n",
      "Test RMSE: 0.0911\n"
     ]
    }
   ],
   "source": [
    "print('Test r2 score: ', r2_score(y_test, y_pred))\n",
    "test_mse = mean_squared_error(y_pred, y_test)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "print('Test RMSE: %.4f' % test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:20.101066Z",
     "start_time": "2019-04-06T13:36:20.089713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet(alpha=0.0009, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
      "      max_iter=3000, normalize=False, positive=False, precompute=False,\n",
      "      random_state=666, selection='cyclic', tol=0.0001, warm_start=False)\n",
      "308\n"
     ]
    }
   ],
   "source": [
    "# Print number of coeffs of fitted model\n",
    "elastic_net_fitted = full_pipe_final.named_steps['reg']\n",
    "print(elastic_net_fitted)\n",
    "print(len(elastic_net_final.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:20.116805Z",
     "start_time": "2019-04-06T13:36:20.103874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'feature_names' (Index)\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(elastic_net_fitted, 'models/elastic_net_final.pkl')\n",
    "%store feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apppendix: Experiment with preprocessing pipe only \n",
    "\n",
    "(**Note to myself**: instead of redefining a preprocessing_pipe I could simply access the preprocessor step from the full_pipe and then call fit_transform() on it, as done in OHE_SMOTENC pipe in the feature_engineering ref code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:36:20.149364Z",
     "start_time": "2019-04-06T13:36:20.119797Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_data = pd.DataFrame(preprocessor_final.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T13:38:54.647440Z",
     "start_time": "2019-04-06T13:38:54.489538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>...</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.439241</td>\n",
       "      <td>-2.059319</td>\n",
       "      <td>-1.102444</td>\n",
       "      <td>0.695955</td>\n",
       "      <td>-0.559401</td>\n",
       "      <td>1.190503</td>\n",
       "      <td>1.130153</td>\n",
       "      <td>1.160692</td>\n",
       "      <td>0.773608</td>\n",
       "      <td>-0.36544</td>\n",
       "      <td>0.175423</td>\n",
       "      <td>0.806540</td>\n",
       "      <td>0.616268</td>\n",
       "      <td>-0.863385</td>\n",
       "      <td>-0.115326</td>\n",
       "      <td>-0.236465</td>\n",
       "      <td>1.103370</td>\n",
       "      <td>-0.23584</td>\n",
       "      <td>0.869888</td>\n",
       "      <td>-0.777852</td>\n",
       "      <td>-1.118721</td>\n",
       "      <td>-0.081748</td>\n",
       "      <td>-0.271242</td>\n",
       "      <td>0.621969</td>\n",
       "      <td>1.233594</td>\n",
       "      <td>0.350952</td>\n",
       "      <td>0.825997</td>\n",
       "      <td>1.032477</td>\n",
       "      <td>0.879823</td>\n",
       "      <td>-0.401530</td>\n",
       "      <td>-0.138279</td>\n",
       "      <td>-0.285155</td>\n",
       "      <td>-0.053222</td>\n",
       "      <td>-0.194815</td>\n",
       "      <td>-0.870215</td>\n",
       "      <td>0.910050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.439241</td>\n",
       "      <td>-0.980800</td>\n",
       "      <td>-0.470414</td>\n",
       "      <td>0.695955</td>\n",
       "      <td>-0.559401</td>\n",
       "      <td>1.122010</td>\n",
       "      <td>1.032060</td>\n",
       "      <td>1.043575</td>\n",
       "      <td>-1.505934</td>\n",
       "      <td>-0.36544</td>\n",
       "      <td>0.644800</td>\n",
       "      <td>0.931397</td>\n",
       "      <td>0.752522</td>\n",
       "      <td>-0.863385</td>\n",
       "      <td>-0.115326</td>\n",
       "      <td>-0.104225</td>\n",
       "      <td>-0.823898</td>\n",
       "      <td>-0.23584</td>\n",
       "      <td>0.869888</td>\n",
       "      <td>-0.777852</td>\n",
       "      <td>-1.118721</td>\n",
       "      <td>-0.081748</td>\n",
       "      <td>-0.271242</td>\n",
       "      <td>0.621969</td>\n",
       "      <td>1.107974</td>\n",
       "      <td>0.350952</td>\n",
       "      <td>0.040435</td>\n",
       "      <td>0.978170</td>\n",
       "      <td>0.749553</td>\n",
       "      <td>-0.401530</td>\n",
       "      <td>-0.138279</td>\n",
       "      <td>-0.285155</td>\n",
       "      <td>-0.053222</td>\n",
       "      <td>-0.194815</td>\n",
       "      <td>0.997546</td>\n",
       "      <td>-1.323708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.105583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.234888</td>\n",
       "      <td>-0.086166</td>\n",
       "      <td>-0.559401</td>\n",
       "      <td>-0.110866</td>\n",
       "      <td>-0.782670</td>\n",
       "      <td>1.290644</td>\n",
       "      <td>0.814614</td>\n",
       "      <td>-0.36544</td>\n",
       "      <td>0.402273</td>\n",
       "      <td>1.857072</td>\n",
       "      <td>1.762695</td>\n",
       "      <td>-0.863385</td>\n",
       "      <td>-0.115326</td>\n",
       "      <td>0.876194</td>\n",
       "      <td>1.103370</td>\n",
       "      <td>-0.23584</td>\n",
       "      <td>0.869888</td>\n",
       "      <td>-0.777852</td>\n",
       "      <td>0.205785</td>\n",
       "      <td>-0.081748</td>\n",
       "      <td>0.429632</td>\n",
       "      <td>0.621969</td>\n",
       "      <td>-0.399472</td>\n",
       "      <td>0.350952</td>\n",
       "      <td>0.381534</td>\n",
       "      <td>-0.968475</td>\n",
       "      <td>0.917033</td>\n",
       "      <td>2.628924</td>\n",
       "      <td>-0.138279</td>\n",
       "      <td>-0.285155</td>\n",
       "      <td>-0.053222</td>\n",
       "      <td>-0.194815</td>\n",
       "      <td>2.118203</td>\n",
       "      <td>-1.323708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.439241</td>\n",
       "      <td>-1.551026</td>\n",
       "      <td>-0.879152</td>\n",
       "      <td>1.478075</td>\n",
       "      <td>-0.559401</td>\n",
       "      <td>0.950778</td>\n",
       "      <td>0.737779</td>\n",
       "      <td>-0.840471</td>\n",
       "      <td>0.823070</td>\n",
       "      <td>-0.36544</td>\n",
       "      <td>0.105918</td>\n",
       "      <td>1.219037</td>\n",
       "      <td>1.405019</td>\n",
       "      <td>-0.863385</td>\n",
       "      <td>-0.115326</td>\n",
       "      <td>0.529053</td>\n",
       "      <td>1.103370</td>\n",
       "      <td>-0.23584</td>\n",
       "      <td>0.869888</td>\n",
       "      <td>-0.777852</td>\n",
       "      <td>-1.118721</td>\n",
       "      <td>-0.081748</td>\n",
       "      <td>-0.972117</td>\n",
       "      <td>0.621969</td>\n",
       "      <td>0.898606</td>\n",
       "      <td>0.350952</td>\n",
       "      <td>0.169640</td>\n",
       "      <td>1.075376</td>\n",
       "      <td>0.868523</td>\n",
       "      <td>-0.401530</td>\n",
       "      <td>-0.138279</td>\n",
       "      <td>-0.285155</td>\n",
       "      <td>-0.053222</td>\n",
       "      <td>-0.194815</td>\n",
       "      <td>1.371099</td>\n",
       "      <td>0.910050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.105583</td>\n",
       "      <td>0.497312</td>\n",
       "      <td>-0.049420</td>\n",
       "      <td>-1.650407</td>\n",
       "      <td>0.382095</td>\n",
       "      <td>-0.521825</td>\n",
       "      <td>-1.371230</td>\n",
       "      <td>-0.840471</td>\n",
       "      <td>0.545121</td>\n",
       "      <td>-0.36544</td>\n",
       "      <td>0.335894</td>\n",
       "      <td>-0.342663</td>\n",
       "      <td>-0.637836</td>\n",
       "      <td>-0.863385</td>\n",
       "      <td>-0.115326</td>\n",
       "      <td>-1.453631</td>\n",
       "      <td>-0.823898</td>\n",
       "      <td>-0.23584</td>\n",
       "      <td>-1.011621</td>\n",
       "      <td>-0.777852</td>\n",
       "      <td>0.205785</td>\n",
       "      <td>-0.081748</td>\n",
       "      <td>-0.972117</td>\n",
       "      <td>-0.962038</td>\n",
       "      <td>-0.608840</td>\n",
       "      <td>-1.055503</td>\n",
       "      <td>-0.590081</td>\n",
       "      <td>-0.968475</td>\n",
       "      <td>-1.109654</td>\n",
       "      <td>-0.401530</td>\n",
       "      <td>-0.138279</td>\n",
       "      <td>-0.285155</td>\n",
       "      <td>-0.053222</td>\n",
       "      <td>-0.194815</td>\n",
       "      <td>0.250442</td>\n",
       "      <td>0.165464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 308 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  1.439241 -2.059319 -1.102444  0.695955 -0.559401  1.190503  1.130153   \n",
       "1  1.439241 -0.980800 -0.470414  0.695955 -0.559401  1.122010  1.032060   \n",
       "2 -1.105583  0.000000  1.234888 -0.086166 -0.559401 -0.110866 -0.782670   \n",
       "3  1.439241 -1.551026 -0.879152  1.478075 -0.559401  0.950778  0.737779   \n",
       "4 -1.105583  0.497312 -0.049420 -1.650407  0.382095 -0.521825 -1.371230   \n",
       "\n",
       "        7         8        9         10        11        12        13   \\\n",
       "0  1.160692  0.773608 -0.36544  0.175423  0.806540  0.616268 -0.863385   \n",
       "1  1.043575 -1.505934 -0.36544  0.644800  0.931397  0.752522 -0.863385   \n",
       "2  1.290644  0.814614 -0.36544  0.402273  1.857072  1.762695 -0.863385   \n",
       "3 -0.840471  0.823070 -0.36544  0.105918  1.219037  1.405019 -0.863385   \n",
       "4 -0.840471  0.545121 -0.36544  0.335894 -0.342663 -0.637836 -0.863385   \n",
       "\n",
       "        14        15        16       17        18        19        20   \\\n",
       "0 -0.115326 -0.236465  1.103370 -0.23584  0.869888 -0.777852 -1.118721   \n",
       "1 -0.115326 -0.104225 -0.823898 -0.23584  0.869888 -0.777852 -1.118721   \n",
       "2 -0.115326  0.876194  1.103370 -0.23584  0.869888 -0.777852  0.205785   \n",
       "3 -0.115326  0.529053  1.103370 -0.23584  0.869888 -0.777852 -1.118721   \n",
       "4 -0.115326 -1.453631 -0.823898 -0.23584 -1.011621 -0.777852  0.205785   \n",
       "\n",
       "        21        22        23        24        25        26        27   \\\n",
       "0 -0.081748 -0.271242  0.621969  1.233594  0.350952  0.825997  1.032477   \n",
       "1 -0.081748 -0.271242  0.621969  1.107974  0.350952  0.040435  0.978170   \n",
       "2 -0.081748  0.429632  0.621969 -0.399472  0.350952  0.381534 -0.968475   \n",
       "3 -0.081748 -0.972117  0.621969  0.898606  0.350952  0.169640  1.075376   \n",
       "4 -0.081748 -0.972117 -0.962038 -0.608840 -1.055503 -0.590081 -0.968475   \n",
       "\n",
       "        28        29        30        31        32        33        34   \\\n",
       "0  0.879823 -0.401530 -0.138279 -0.285155 -0.053222 -0.194815 -0.870215   \n",
       "1  0.749553 -0.401530 -0.138279 -0.285155 -0.053222 -0.194815  0.997546   \n",
       "2  0.917033  2.628924 -0.138279 -0.285155 -0.053222 -0.194815  2.118203   \n",
       "3  0.868523 -0.401530 -0.138279 -0.285155 -0.053222 -0.194815  1.371099   \n",
       "4 -1.109654 -0.401530 -0.138279 -0.285155 -0.053222 -0.194815  0.250442   \n",
       "\n",
       "        35   36   37   38   39   40   41   42   43   44   45   46   47   48   \\\n",
       "0  0.910050  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
       "1 -1.323708  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2 -1.323708  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.910050  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "4  0.165464  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   49   ...  258  259  260  261  262  263  264  265  266  267  268  269  270  \\\n",
       "0  1.0  ...  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "1  1.0  ...  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "2  1.0  ...  1.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  1.0  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  1.0  ...  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   271  272  273  274  275  276  277  278  279  280  281  282  283  284  285  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   286  287  288  289  290  291  292  293  294  295  296  297  298  299  300  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  1.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  1.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0   \n",
       "\n",
       "   301  302  303  304  305  306  307  \n",
       "0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  1.0  0.0  0.0  1.0  \n",
       "3  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
       "4  1.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 308 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look\n",
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "464.86px",
    "left": "1092.86px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
